greedy matching
cuda 9.2
paragraph embedding
general text
edge probing
imagenet pre-trained cnn
sentence-pair completion example
rm idf
m-bert
i2b2 named entity recognition
word bigram matching
word feature
bert-large
max-pooling
re-ranker
cross-encoder
supervised data
abstract
benchmark dataset
masked language modeling objective
training set size
news
v^ l + 1 _ m
representation-learning objective
english-specific word piece vocabulary
unmatched image
m2
text classification
syntactic task
task-specific branch
held-out training data
paraphrase
feature-based and fine-tuning
attention probing classifier
triviaqa fine-tuning data
attention score
aggregate representation
architecture design
clinical nlp research
coreference
linguistic structure
inspection-based probing
natural language inference
probing attention map
mimic
in-house dataset
sts dataset
cls vector
nli data
fine-tuning setup
vqa
note-type specific embedding model
hidden state
full-sentence
nlp task
de-id
language generation evaluation metric
gtx-1080ti gpu
smart batching strategy
private held-out test
bert optimization hyperparameters
unicoder-vl
argument facet similarity
f1 score
start vector
overfitting
bert model fine-tuned on paraphrasing
binary classification model
supervised classifier
english-derived vocabulary
ground-truth rois
infiniband
weight vectors w
pre-training task
segment-pair
correlation
batche size
encoding
pre-training of generic feature representation
i2b2 2010 task
visual-linguistic corpus
accuracy
question-answering
v1.1
higher-level representation
optimization step
wikipedia dataset
contextual embedding model
scalar mixing
synonyms
bidirectional encoder representation
squad v2.0 development set
character level
spr
average glove embedding
text generation
temporal modifier
annotated reference sentence
diverse corpora
semantic role labeling
cls element
non-contextual word
itemize mr
siamese network structure
ud
neural network
query-passage pair
model architecture bert 's model architecture
task-specific architecture
re-ranking task
widely-used vqa v2.0 dataset
determiner relation
importance weighting
transformer attention
average bert embedding
attention masking
multi-reference bleu
wals feature
ltr system
china
pairwise ranking formulation
early stopping
masked video clip
inverse document frequency score
region-of-interest
m-bert 's multilingual representation ability
differential score delta^ _ tau
w_k alpha^k _ ij
sentence embeddings method
clinical note
classification objective function
sst
bidirectional self-attention
pos
feature representation
de-id context
per-token level
linguistic feature
linguistic domain
supervised downstream task
conll-2003 named entity recognition
network training
cls-token
poss relation
mlm-based loss
scarce data
data preprocessing
fuzzy matching
bert training procedure
text-only corpus
syntactic ability
nvidia tesla v100 gpu
mixed clinical discharge summary
linguistic clue
induced language model structure
span prediction method
deep bidirectionality of bert
attention
auxiliary next sentence prediction
contextualized token embedding
deps .
afs
bert 's vector representation
feature vector
de-identification task
inference
biobert
heuristic tokenization rule
non-english language
summary statistic
byte-level bpe vocabulary
trec dataset
clinical bert
similarity notion
adversarial paraphrase dataset
pytorch-transformer
massive-scale target dataset
mlm objective
image captioning metric
predicting randomly masked rois
validation
labeling task
conceptual captions dataset
naive classifier
sts specific training data
sports team
weight vector
official pre-trained bert model
average correlation
rm tf
cder
attention distribution
non-null answer
complex co-occurrence statistic
siamese bilstm network
cls representation
masked word prediction
nearest neighbor accuracy
perceptual level
contextualized word representation approach
semantic similarity search
siamese network architecture
roberta
rm tf-rm idf measure
toolkit
transliteration-writing
human correlation of bertscore
swag dataset
left-only constraint
arabic script
text-only dataset
high-dimensional representation
bert re-ranker
tag prediction
p^
plus-one smoothing
string matching
o score computation
text generation model
pre-training of vl-bert
token vocabulary
model perfomance
devanagari script
relation classification
doc-sentences format
metric ranking
character-level bpe vocabulary
baseline esim
text generation system
weighted sum
contextualized word embedding
vqa v2.0 dataset
masked language model objective
squad 1.1 problem definition
general-domain pretrained elmo model
hyperparmeter tuning
hyperparameters
english wikipedia dataset
masking word
deep encoder
f1 measure
per-layer classifier
race test set
linguistic content
human score
cbt
sentinel begin sentence token
monolingual corpora
non-visual element
squad
qualitative embedding comparison
robustly optimized bert approach
semeval 2014 task 14 task
full string match
yisi-1
corpus
encoder layer ell
embedding feature
question section
image
annotation convention
penn treebank
term frequency
xlnet _ large
computer vision task
drqa
de-id task
distant dependency
tf-idf
token-level task
m context vector
natural language query
subwords unit
left-to-right generation of next sentence word
general language understanding evaluation benchmark
relevance annotation
long contiguous sequence
nominal mention
meaning-preserving compositional diversity
clinical operation
masking
pre-training of visual-linguistic task
semantic textual semilarity benchmark
pretraining method
sentence prediction task
squad 2.0 task
intel i7-5820 k cpu
scalar mixing technique
perturbed autoregressive language modeling
mlm-based pre-training technique
object category classification
supervised task
learning loss
pretrained model
pre-trainable generic representation
lm
non-contextual word embedding
alignment between linguistic content
bert pre-training
absolute location word
bookcorpus
_ pos
ner
syntactic structure
triplet objective function
leave
adjective / noun order
reading comprehension from examination
bert_base
feature map
semantic task
single-sentence classification
linguistic meaning
sts data
typological similarity
n-gram matching
semantic textual similarity
triplet loss
structural probe
sep token
natural language text generation
wikipedia page
q rightarrow a
lexical gap
conll data
position embedding
word-sensitive weight
forum
semantic equivalence
russian
vcr
named entity recognition
i2b2 2014
p^ _ tau
multi-task finetuning procedure
attention probe
computational cost
inverse document frequency
high-level information
question-answer page
mixed precision floating point arithmetic
i2b2 2012 entity extraction challenge
human caption
coco
sent-eval
wikipedia article
translation system
localizable region
span predictor
metric quality
classification score
human judgment data
mask token
machine reading compression
bi-lstm layer
computational efficiency
consts .
index structure
stacked transformer layer
task-specific model
english-language corpora
word-level task
massive-scale dataset
perplexity
absolute pearson correlation
static masking
rte
classification token
sentence-pair format
real name
absolute pearson correlation lvert rho rvert
fine-tuning approach
cls-token output
neural model
language model pretraining
cross-modal architecture
token-level computation
auc
shuffled sentence-level corpus
imagenet classification
web content
hindi word
abstraction
vqa task
visual-linguistic downstream task
modest data augmentation
word2vec
separator token
sentence-level representation
glove embedding
meteor++ 2.0
webtext corpus
probing model
layer
left-to-right model
detection branch
answer span
multi-layered context
pre-trained bert model
system-level metric
monolingual corpus
senteval toolkit
objects of preposition
w^ l + 1 _ m
mask element
argm-tmp
searchqa
external resource
dependent
cls-token strategy
multi-nli dataset
fixed-size sentence embedding
semantic textual similarity task
pre-trained transformer network
nonspecific embedding
task-specific model architecture
fine-tuning
hidden layer
content feature
cls token representation
bert cls-token output
mean
conll category
adam
pre-trained vl-bert model
bidirectional conditioning
x _ setminus i
v2.0
long-range dependency
concatenation mode
summarization evaluation
semantic structure
subspace
fine-tuning mechanism
skip-thought
external stemmer
reformatted wnli data
word-level vector representation
network structure
clinical text de-identification
subject / object / verb order
similarity
deep language model
pixel
meteor
combinatorial explosion
superficial form of generalization
cross-lingual transfer
task-like corpora
reference sentence
unsupervised method
answer
similarity of scene graph
non-contextual embedding method
bpe vocabulary size
quora
non-contextual embedding
bert pretraining approach
next-sentence prediction
deep transformer architecture
wmt15 human judgment data
grounded commonsense inference
dependency modeling
end vector e
token n-gram
middle-school and high-school setting
bowl
pre-trained language representation
snli
natural language understanding system
human design
superglue
linear layer
org
cc-news
scalar parameters gamma _ tau
hyperparameter
sentiment prediction
bert cross-encoder
exactp_1
language understanding system
negative candidate phrase
word swapping
answer prediction
softmax layer
rtl model
fine-tuning stage
ranking ability
coco image
dependency structure
adam optimizer
tree structure
per-token contextual vector
wmdo
structure of m-bert 's feature space
code-switching
downstream task data
wmt18 segment-level human judgment
classification loss
fairseq
sentence embedding method
shared representation
prediction
generic clinical text
character n-gram
syntax-aware attention
fine-tuning data
sentence pair
left left language model
n-gram-based metric
linear decay of the learning rate
elmo
training strategy
sine function
fast r-cnn
task-specific loss function
siamese
annotated dataset
summarization system
hidden unit
learning rate
bert embedding
poly-encoder
m-bert 's multilingual representation
zero lexical overlap
expected layer
coco 2015 captioning challenge
diin
auto-encoder objective
sentence structure
variable batch size
contextual embedding
visual appearance feature
deep bidirectional representation
model architecture bert 's
english text
pre-trained sentence embedding model
hidden feature activation
adam epsilon term
replication study of bert pretraining
multilingual representation
vl-bert: pre-training of generic visual-linguistic representations
linear structure
constrained self-attention
earth mover 's distance
mimic-iii v1.4 database
i2b2 named entity recognition task
entity labeling
unannotated text
syntactic attention head
weakly labeled sentence triplet
spice
semantic similarity
pre-normalized vector
raw mimic
linear warmup
nli dataset
qqp data
faster r-cnn + resnet-101
pos tag
ablation study
coco validation set
annotated transformer
query
i2b2 2006 1b de-identification
clinical specific contextual embedding
contextual model
natural language processing system
tokenized reference sentence
monolingual hindi
document-level corpus
i2b2 2014 task
semantic parse
transformer encoder
classifier layer initialization
ms marco training data
concatenated text pair
cross-lingual transfer scenario
wmd
bert 's attention head
twelve-layer bert
english wikipedia paragraph
referring expression comprehension
pre-trained faster r-cnn
situations with adversarial generation
ms marco passage retrieval task
regression model
quora question
eintailment
sentence representation
attention-based probing classifier
bert pretraining
left-context-only model
xlnet
bulgarian
passage re-ranker
reddit
embedding representation
epsilon
generic feature representation
decoupled representation
sts
attention module
ir-net
unified architecture
js
binary classification loss
denoising auto-encoder derived objective
conll-2012 dataset
universal encoding scheme
clinical text dataset
segment-level
cls token representation c
gpe
positive referent phrase
negative euclidean distance
weight matrices w
cluster id
low-resource task
text distribution
word-word attention map
maximum scoring span
domain-specific model
right left language model
attention dropout
document boundary
sep
multinli accuracy
per-layer metric
biomedical text
bert pretraining procedure
mattnet
general purpose natural language processing model
bidirectional cross attention
clustering mechanism
conditional language model
low similarity language
crowdsourced question / answer pair
pos tagging
en-bert
joint probability distribution
cudnn
run-time overhead
qnli
margin ranking loss
automatic evaluation metric
token-level hidden state
2016
masked word
visual content embedding
turkish
cider
segment
heuristic
bilstm
bert
out-of-order
derived representation
rich pre-training task
machine translation model
sentence-pair classification task
word-level
faster r-cnn
task-specific metric
low-level syntax
regression function
stock bert model
continuous runtime
embedding model procedure
bm25
predicting randomly masked word
human-defined syntactic relation
shallow statistic
random guess
monolingual-only and code-switched supervision scenario
clinical note corpus bert language model
xlnet architecture
common crawl monolingual dataset
matching similarity score
sentence-pair
qqp
structural analysis
fine-tuning based representation model
neural machine translation
12-layer model
pawsqqp development set
transfer
dependencies among linguistic content
detection score threshold
semantic relatedness of sentence pair
situations with adversarial generations dataset
bert out-of-the-box
image caption generation
design decision
mimic text
sentence-bert: sentence embeddings using siamese bert-networks
deep unidirectional architecture
language
brevity penalty
language modeling objective
fast r-cnn detector
position independent error rate
similarity computation
fine-tuning scheme
rare word
winograd schema
syntax-aware model
task-specific annotation
dataset size
urls
network architecture
training
average dev set accuracy
semantic segmentation
visualbert
pawsqqp
bert 's training
ms marco
semantic overlapping
positive training example
classification approach
k^ l + 1 _ m
top leaderboard system
transformer attention module
computational cost pre-processing
deep bidirectional transformer
sentence-pair regression
alpha^k _ ij
linguistic characteristic
knrm
paraphrase detection
biomedical research article
random initialization
shared space
machine translation
fine-tuning bert
dynamic masking
v100 gpu
jensen-shannon distance
token similarity
discussion forum
uncased bert model
automatic metric
contextualized word representation
finetuning procedure
synthetic mask
beer
task-agnostic metric
pipeline order
zero-shot transfer
hidden representation
sentinel phi token
regression objective function
vector space
denoising auto-encoder
afs corpus
maximal document context
relevant and non-relevant passage
afs data
de-identification dataset
pre-computed candidate embedding
nsp task
training dataset
clinical radiology note
glue dataset
document
triplet network
ms marco passage re-ranking task
reading comprehension from examinations task
vl-bert_base
bert-style pretraining
left-to-right architecture
typological feature
i2b2 task
visual-linguistic bert
higher-commonsense understanding
semantically-critical ordering change
cosine similarity
comma
de-identified source text
pre-trained transformer
matching heuristic
sentence-image-relation level prediction
movie reviews snippet
fully-connected layer
model analysis technique
dev set
natural image
sentiment analysis
data drift
concatenation
de-identification
clinically oriented bert model
scalar parameter
linguistic phenomenon
right-side context
rm idf importance weighting
distributed data parallel training
pre-trained model
masked roi
r^ h
pre-trained word / positional embedding
semantic textual semilarity
vl-bert_base model
context-sensitive feature
blend
graph-based dependency parser
glue task
ner label set
elmo system
dependencies in sentence word
pre-trained contextual embedding model
neural coreference system
fine-tuned model
multi-task finetuning
bert 's model architecture
mean-squared-error loss
duet
classifier
self-training method
hybrid system
wikipedia segment
image captioning
gpt-2
ruse
written text
private held-out test data
fine-tuning task accuracy
triplet network structure
matching
infersent model
hidden vector
mnli
unidirectionality constraint
sroberta
unlabeled data
pre-training objective
context-aware neural machine translation system
transformer
non-neural method
segment-level information
evaluation metric
butd
single-task training data
unlabeled text corpus
gradient-based measures of feature importance
tokenization
contextual encoder layer
sentiment information
sts system
fast r-cnn feature
story
random pairing
positional attention head
word edit distance
sentence relationship
linguistic information
architecture bert 's model architecture
self-attention
dot product
wikipedia
entity embedding
movie scene
langle question
analyzing attention
labeled training example
linguistic decision
model size
multi-class classifier
p ^
sentinel phi marker
synonym lexicon
ell
peak learning rate
r_bert
model output
stopping
common word
wmt16 dataset
vision
word stem
what does bert look at? an analysis of bert's attention
human judgment score
unmatched sample
vqa dataset
crf layer
roi classification
linguistic pattern
matching process
monolingual english
attn + glove probing classifier
agreement task
end-task performance
system-level human judgment
billion word benchmark
i2b2 2012
language representation model
image recognition
beta_2
squad v1.1
test f1
e_eval
log-likelihood
feature of bert
structure of language
lm perplexity
dependency parsing
sentence similarity
openwebtext
qa
general domain text
downstream visual-linguistic task
english news article
negative sentence n
triviaqa data
mean reciprocal rank
vilbert
notnext
cross-attention
softmax
computational runtime
siamese / triplet network architecture
sigmoid classifier
mednli natural language inference task
transfer learning task
adversarial generation
average entropy
masked language model
uas
determiners of noun
mr
semantic information
note-type specific corpora
network
drmm
generation
q rightarrow ar task
syntactic abstraction
lxmert
pretraining step
linear transformation
input representation
wmt18 en-de test set
task-specific supervised training data
fine-grained semantic distinction
visual-linguistic clue
pre-trained bi-lm size
natural language generation
i2b2 2010
evaluation task
multi-genre nli dataset
numpy
multi-modal transformer attention module
python dictionary lookup
natural language phrase
pre-training bert
f1
query-based passage re-ranking
reading comprehension dataset
readily available unannotated text
sgd training
transliterated version
wmt16
token embedding
probing classifier p _ tau
left-to-right language model pre-training
general language understanding evaluation
pre-trained checkpoint
left-to-right
ruse model
bert usage
visualization tool
span prediction
fine-tuning based approach
monolingual english data
pre-trained word embedding
model prediction
cross entropy loss
proto-role
passage re-ranking
refcoco+
indirect supervision
left context
parsing
f_bert
image recognition task
internal vector representation
masked language model training objective
auxiliary next sentence prediction loss
mlm
gradient accumulation
max-over-time
cloud tpu
document encoder
evaluation objective
language model surprisal
adversarial generations dataset
multilingual vocabulary
doc-sentence
supervised learning
unidirectional language model
answer justification
limited hyperparameter sweep
bert implementation
positive sentence p
ldot
shared word piece vocabulary
synthetic de-id
domain mismatch
10-fold cross-validation
differential score delta^
external training data
transformer model
model architecture bert
numerator
multi-modal information
answer rangle
neutral
question-answering data
cluster
a embedding
probing vector representation
entity type
left lms
training of bert
local syntax
hierarchical structure
fixed-offset baseline
hidden dimension size
chinese task
stochastic training
data size
non-contextual baseline
masked language modeling
tensor2tensor library
pre-trained bidirectional model
eed
recall r_bert
p _ tau
generic pre-trainable representation
probing classifier
token representation
large corpora
de-id corpus
lexical overlapping
clinical domain
trec-car corpus
isnext
rois
multilingual bert
layers of convolution
unmatched caption pair
antecedent selection accuracy
processing time
mrr@10
natural language corpora
10-fold cross-validation setup
syntactic behavior
mednli
r2c
syntactic dependency
per-token representation vector
post-bert method
t_i
chrf++
human ranking
question
reading comprehension task
leic
ms marco corpus
machine translation evaluation
encoder-decoder architecture
visual genome
sacrebleu
fully connected layer
ordering
wmt16 datasest
emd-based approach
strict n-gram matching
gradient
pre-trained sentence encoder
refcoco+ dataset
predicting similarity
linguistic word
information retrieval community
coco captioning challenge
coref .
computer vision
english examination
roi-level prediction
transfer from supervised task
measures of feature importance
oplus
pubmed article abstract
nist
joint pre-training
dependency
raw text
contextual token representation
fine-tuning code
captioning-only dataset
cross-attention bert
natural progression
coreference link
dot product w _ k
cosine-similarity
words probe
nsp loss match
token prediction
mlm model
i2b2 2014 7a de-identification challenge
private training data
sms
syntactic head
glue task development set
smart batching
czech
machine translation training
agent arg0
large-scale dataset
wmt18 metric evaluation dataset
optimization speed
u-v
representational capacity
code-switched data
semantic relatedness
2012 task
computation of sentence embedding
glue
heavily-engineered task-specific architecture
triplet objective
model architecture
rm idf weighting
nonlinear transformation
classifier output
decent coreference resolution
xlnet _
transfer learning
wave
image captioning system
span index
medically oriented wikipedia article
bert-as-a-service-repository
random masking
m^th attention head
pre-train / fine-tune mismatch
next sentence prediction
classification element
bert _ large
softmax classifier
multi-task objective
pre-training
no-op
cnns
glue score
bert _ large architecture
probing task
context-sensitive embedding
visual element
fine-tuning learning rate
nlp community
x_1
sts benchmark dataset
clinical bert instantiation
williams test
finetuning
semantic phenomenon
text generation evaluation
byte-level bpe
positive example
de-identified text
edit operation
autoregressive pretraining
language modeling
score function
right context
trec-car 2017
automatic annotation
shallow semantic parse
loss function
colon masked language modeling
attention head 's function
neural sentence embedding method
language model pre-training
bertscore
nlp
statistical analysis
12 layer transformer
low-level linguistic feature representation
word embedding vector
bert encoder
classification layer
cross-script transfer
pre-trained representation
clinical ner task
word piece overlap
visual-text dataset
faster r-cnn detector
vl-bert
dialog
similarity measure
non de-id task
universal dependencies
generalization
probability space
jensen-shannon divergence
internal representation
sentence embedding
passage re-ranking with bert
activation vector
objective function
massive-scale conceptual captions dataset
neural ranking model
segment-level human judgment
bpe
bert contextual embedding
monolingual wikipedia corpora
masked language modeling pretraining objective
edge probing approach of
adaptive attention weight
pre-trained neural network
down-stream task
nlp benchmark
german sentence vector
spr task
general-purpose linguistic representation
sentinel phi symbol
japanese
normalization
generic representation of vl-bert
optimization objective
human annotated negative reference sentence
synthetically-masked phi
multidimensional scaling
sts tasks 2012-2016
nlp system
commoncrawl data
structured-prediction task
manhatten / euclidean distance
tagging task
senteval transfer task
english
hidden vector c
exactp_n
german
pobj
visual-linguistic task
biomedical domain
low-resource language
unlabeled text
no-answer span
task-agnostic baseline
stem matching
hidden size
cls
f1 f_bert
snli dataset
euclidean distance
concatenation method
visual clue leakage
sts benchmark
mnli single-task model
cross-topic setup
pre-trained architecture
l2 weight decay
mimic corpus
pawswiki
fuzzy matching between synonyms
syntax
contextual representation
tv network
masked language model pretraining
bidirectional pre-training
adversarial paraphrase dataset paws
triplet dataset
triviaqa
sequence position embedding
ontonotes development set
directed parse tree
probability distribution
deliminator token sep
rouge-l
clinical biobert
task-specific grounding
pretraining procedure
bert_large model
skipthought
rouge-n
p_bert
visual question answering
4-d vector
tpu v3-8
kendall rank correlation tau
transformer network
squad v1.1 development set
clinical de-identification task
sentence
downstream task
self-attention layer
self-supervised training
token embedding channel
ensembling
word bigrams
training objective
imagenet pre-training
medical natural language inference task
words probing classifier
stretching effect
attention mechanism
unsupervised task
visual feature embedding term
generative pre-trained transformer
exactr_n
european language
zero-shot cross-lingual model transfer
pooling strategy
bert representation
meant 2.0
entity
transformer block
character-and word-level representation
trec-car dataset
deltableu
visual geometry embedding
training format
n-gram overlap
single-stream unified model
syntactic entity information
ltr
regression method
contextualization module
random seed
masked lm
prediction accuracy
clinical operations context
exact matching score
training time
appearance-based description
openai gpt
geometry of contextual embedding
sbert sentence embedding
diverse natural language understanding task
binarized next sentence prediction task
vcr benchmark
visual content information
supervised sts system
period
glue benchmark
classification
transliterated text
r^h
segment-level correlation
de-id challenge data
automatic pipeline
l_2 weight decay
question answering task
self-attention head
quora question pair corpus
cpu power
cumulative scoring
element-wise difference
model architecture bert 's model
swapping of cause clause
sequence tagging architecture
logistic regression classifier
lvert rho rvert
dutch
spoken text
sentence prediction nsp
masking pattern
language-specific information
finetuning model
embedded feature
script-corrected input
bert _
detection score
web data
direct supervision
multi-head attention
nsp loss
end-task accuracy
dropout probability
bert / roberta
lstm
gpus
area under roc curve
biobert specific embedding
high-level semantic information
contextual word embedding model
categorized rois
iob format
shallow concatenation
monolingual input
dependencies among visual content
bookscorpus dataset
coreferent mention
cross-lingual generalization
precision
pretraining objective
hyperparameter space
system-level correlation
clustering
sts task
pre-trained text encoder
end-task labeled data
potential function
qnli task
synthetically non de-identified task text
non de-id clinical nlp task
nlp model
reference token sentref_i
linguistic element
start vector s
automatic evaluation of natural language generation
massive visual question answering data
subject-verb agreement
infersent
wordpiece embedding
death penalty
sts tasks 2012
architecture bert
labeled data
clinical-specific embedding
referring expression
averaging glove embedding
constituent
minibatch
alexnet
structural similarity
chrf
entropy
negative manhatten
useful linguistic information
computational resource
12-layer bert-base model
evaluation toolkit
pooling operation
no-training-required classifier
cs hindi / english ud corpus
task agnostic
training procedure
classical ir technique
layer activation
language processing
pre-training technique
bertscores
large-scale semantic similarity comparison
facet
neural method
rm idf score
clinical bert model
masked language modeling task
esim + elmo system
masking operation
non-noun
human judgement
edge probing suite
complexity
multi-layer bidirectional transformer
task-specific finetuning approach
transliteration signal
architecture bert 's model
feature-based approach
j _ neg
ltr model
unicode character
roi
bleu implementation
monolingual , standard-script corpora
character
language inference
multilingual training objective
significance
question-passage pair
multilingual objective
imagenet
srl
glove probing classifier
spearman 's rank correlation
nli-format data
task corpora
deep , contextualized language model
tensorflow hub version
contextualized embedding
visual word
encoder
specificity
wikipedia document
information retrieval
part-of-speech tagging
natural sentence
unified single-stream architecture
cls token
lexical similarity
sentence classification
ell + 1
bounding box
one-hidden-layer network
sentence-pair regression task
video
bootstrap re-sampling
abstractive text summarization
english task
image-based visual-linguistic task
24-layer
computational overhead
ms marco dataset
overlap
aggregate sequence representation
bidaf
regression
tokenizer
stanford dependency
bertscore: evaluating text generation with bert
human judgment
inner product
masked roi classification
ir
corpus diversity
for-loop
sbert
coreference resolution
supervised method
exactr_1
pubmed central article full text
wmt18
spearman correlation
bidirectional pre-trained model
generic representation
pre-training visual-linguistic representation
recall
pre-trained encoder
bert 's output
evaluation set
part-of-speech tag
gpt
nsp
discharge summary bert
segment embedding
videobert
paraphrasing
natural language processing task
computation speed
meaning-preserving lexical diversity
stanford natural language inference dataset
end vector
analysis method
cr
task-specific input
transliterated language model pretraining
high-entropy attention head
ner f1 score
pos-labeled urdu
behavioral study
sub-word
swapping of effect clause
word attention head
roberta: a robustly optimized bert pretraining approach
xlm
subword vocabulary
hipaa phi dataset
data distribution
task discharge summary bert
sentence prediction
multinli
dot product w
entity-typing decision
general bert
conll-2003 named entity recognition task
grammatical ordering
batch size
deep bidirectional model
attn probing classifier
race
word scrambling dataset
dgx-1 machine
trec-car
passage ranking
hypothesis-premise pair
edge
weight matrix
organization
main model
activation
internal vector representations of the model
surface-level pattern
fine-tuned bert
bert rediscovers the classical nlp pipeline
24-layer bert-large model
siamese dan
fasttext
average performance
beta_1
piece
phi
precision p_bert
sentence-bert
bert network
classification task
clinical clinical biobert model
bert output layer
action recognition
bert-base
mean pooling
b embedding
translation pair
reasoning module
ablation
cross-entropy loss
question answering dataset
multi-class cross-entropy loss
self-attention mechanism
paraphrase detection training data
mask
word order
memory network
bert_large
linear decay
coco data
m-bert zero-shot
ulmfit
single-task finetuning
unknown word
img token
word english corpus
scalar parameters gamma _
private held-out
geometry location
bert: pre-training of deep bidirectional transformers for language understanding
bilstm architecture
clinical text
argument facet similarity corpus
visual clue
estonian
visual commonsense reasoning
maximum n-gram length
intermediate layer
string-matching baseline
coarser granularity
byte-pair encoding
mimic note
category label
max
encoder weights
coco dataset
pre-processing task
word error rate
unknown word piece
stsb
configuration roberta
time dimension
learning rate warmup
co-pacrr
referring expression comprehension task
transformer architecture
hospital
argument similarity dataset
stanford question answering dataset
wmt17
high-level information like predicate-argument relation
paraphrase classification
code-switching text
roberta model
bert _ base
linguistic knowledge
pre-trained embedding
higher-order cognitive understanding
sick-relatedness dataset
quality
feature
contextual word embedding
concatenated document segment
visual content
n-gram model
entailment
specialty corpora
squad 1.1
mlm-based training of bert
optimal matching
task-specific parameter
partition function
how multilingual is multilingual bert?
gelu activation function
token-token attention map
data augmentation
pos zero-shot accuracy
labeled dataset
integrated task-specific architecture
dot product w _
common wals feature
reduced sequence length
relevance score
head word match
large-scale task
clinical nlp task
universal sentence encoder
similarity score
regions-of-interest
left-to-right and right-to-left representation
x_n
bidirectional model
sentbleu
e_train
hierarchical clustering
system-level and segment-level correlation
multilingual
external paraphrase resource
spanish
shared multilingual representation
qualitative analysis
differential score
question-type classification task
feature design
separation element
geopolitical entity
bert transformer
tuning
visual feature embedding
image captioning task
pre-training general language representation
single-modal network
bert _ score
dependency relation
element-wise u*v
_ neg
glove
candidate token senthyp_j
system-level score
unsupervised pre-training
pre-training task of masked roi classification
machine-learned attention weight
pre-trained deep bidirectional representation
uas score
mrpc
macro-averaged pos accuracy
alpha _ ij
non-exact matching
pearson correlation r
log-potential term
bert attention head
text pre-processing procedure
unmasked word
positional embedding
pre-trained faster r-cnn object detector
adversarial example
encoder network
surface-form similarity
visual grounding
object detection
block reordering
adversarial paraphrase classification
training bidirectional representation
decent uas
pre-trained parameter
swag
adaptation of bert
scibert
publicly available clinical bert embeddings
rescaling procedure
qa dataset
object detector
vector representation
private dataset
caption generation
natural language processing
degenerate text-varnothing pair
transferability
softmax-normalized dot product
ms marco passage ranking dataset
greedy
discrete pipeline
latin script
semantically meaningful sentence embedding
argumentative excerpt
semantic relation
cs training set
discharge summary
q^ l + 1 _ m
jackroos
semeval
qanet
score readability
visual feature
visual domain
cross attention
domain-specific contextual embedding
neural network component
syntactic relation
right lms
token-level classifier
i2b2 2006
admission
alice arnold article
attention pattern
word embedding
multi-modal
pre-training procedure
caption
semeval 2014 2015 14 task
z_+
relation networks
uncompressed text
cross-topic evaluation
tensorflow
nli
lexical overlap
gay marriage
bert attention output
fixed sized sentence embedding
de-id challenge
p ^ _ tau
surface form of the reference
discharge
function word
alignment between visual content
comparison of metric
entity overlap
clinically fine-tuned biobert
bert-based model
single layer neural network
pearson correlation
adversarial paraphrase
scalar mixing weight
attention weight
biomedical embeddings
video clip
attention behavior
large-scale reading comprehension dataset
mixing weight
finetuning setting
paw
contextual clinical
senteval
generating sentence embedding
ground-truth annotation
quasar-t
sentence embeddings u
sequence tagging
soft measure of similarity
squad training data
sentiment task
pre-training data
gaussian distribution
pre-training process
visual information
part-of-speech
multi-task fine tuning
nlp pipeline
independent sentence embedding
n^2
syntactic tree
hyperparameter choice
end-task model parameter
conceptual caption
bert network structure
left-to-right language modeling
learned function
ranking approach
sentence-level task
unsupervised way
elmo embedding
rule-based system
passage ranking task
question-answering pipeline
task-specific model design
supervision
elmo model
hindi
word embedding parameter
machine translation metric
gun control
vocabulary memorization
span loss term
cross-modal transformer
indicator function
multi-layer bidirectional transformer encoder
average accuracy
syntactic information
bookscorpus
pytorch
task-specific architecture design
2003 set
instance segmentation
unsupervised learning
recursive neural network
right-branching baseline
generic visual-linguistic pre-training
cosine function
pre-trained language model
convolutional neural network
bing search engine
generation evaluation metric
case-preserving wordpiece model
paraphrase table
colon masked roi classification
language model
human evaluation
edit distance
softmax cross-entropy loss
bidirectional encoder representations from transformer
biomedical nlp task
bert head
tokenized candidate
transformer encoder architecture
huggingface
semantic abstraction
nsp objective
pre-trained elmo model
sequence information
paraphrase adversary
layer-wise learning rate schedule
prior word embedding
adversarial paraphrase detection task
model
corpus statistic
bookcorpus dataset
sequence positional embedding
robustness
loss term
sensitive attention head
task-specific need
pre-training vl-bert
human-curated treebank
named entity information
sentence word
english bert model
iterative attention
trec-car 2017 organizer
attention map
testing
i2b2 2010 concept extraction
semantic proto-role
finnish
random restart
training data
large batch training
full word
model ensembling
iterative
question answering
sentence-image relationship prediction
training data generator
computer vision research
static mask
absolute f1 score
external tool
contradiction
fine-grained output
commoncrawl news dataset
semantic role
memory
mini-batch
physician note
pretraining
bilstm-layer of infersent
nvidia v100 gpus
s^n _ x
12-layer cased multilingual model
massive dataset
pos-tagged devanagari word
english syntax
b2t2
answer generation module
3-way softmax-classifier objective function
bleu
penn treebank dev set
best-performing model
anaphora
model selection
semantic search
output prediction
social media dialog
rm tf-rm idf weighted n-gram
language modeling task
cloze task
gpu
ad-hoc task-specific module
embedding matching
clinical narrative
non-clinical biomedical text
static word embedding
english wikipedia
generic feature representation of vl-bert
pre-trained vl-bert
baseline pretrained roberta
byte-pair tokenization
universal dependencies data
fine-tuning data shuffling
hierarchical information
non-cross-lingual ner
permutation
inference computation
binary classifier
edge probing approach
m1
spoon
distance feature
negative example
stanford dependencies
sentence prediction objective
model analysis
bert model
embedding model
documentqa
left-to-right lm
full-length sequence
semantic task of coreference resolution
m-bert model
attention head
glue leaderboard
visual-linguistic dataset
pair regression task
j _ pos
conll-2002 set
vcr dataset
left dog
section
syntactic distance
masked language model pre-training objective
n-gram
m-bert 's ability
number / gender / person match
low-level decision