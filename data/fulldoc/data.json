["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.", "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.", "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.", "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\"~(MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pre-trains text-pair representations. The contributions of our paper are as follows: itemize[leftmargin=1em] We demonstrate the importance of bidirectional pre-training for language representations. Unlike , which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to , which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert. itemize", "There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.", "Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. To pre-train word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context.", "These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. To train sentence representations, prior work has used objectives to rank candidate next sentences , left-to-right generation of next sentence words given a representation of the previous sentence, or denoising auto-encoder derived objectives.", "ELMo and its predecessor generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. shows that the cloze task can be used to improve the robustness of text generation models.", "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text .", "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark. Left-to-right language modeling and auto-encoder objectives have been used for pre-training such models.", "There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference and machine translation. Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet.", "We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure will serve as a running example for this section.", "A distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.", "Model Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in and released in the tensor2tensor library. Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to as well as excellent guides such as \"The Annotated Transformer.\"", "In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERT_BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT_LARGE (L=24, H=1024, A=16, Total Parameters=340M).", "BERT_BASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.", "Input/Output Representations % Claim we shared the structures To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., langle Question, Answer rangle) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \"sequence\" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.", "We use WordPiece embeddings with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure, we denote input embedding as E, the final hidden vector of the special [CLS] token as C in R^{H}, and the final hidden vector for the i^{ rm th} input token as T_i in R^H.", "For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure.", "Unlike and , we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure. Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \"see itself\", and the model could trivially predict the target word in a multi-layered context.", "In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \"masked LM\" (MLM), although it is often referred to as a Cloze task in the literature. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders , we only predict the masked words rather than reconstructing the entire input.", "Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace \"masked\" words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, T_i will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix.", "Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure, C is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section that pre-training towards this task is very beneficial to both QA and NLI.", "The NSP task is closely related to representation-learning objectives used in and . However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.", "Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.", "Fine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks---whether they involve single text or text pairs---by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.", "For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text- varnothing pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.", "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. We describe the task-specific details in the corresponding subsections of Section. More details can be found in Appendix.", "The standard pattern in many NLP models is to independently encode text pairs before apply bidirectional cross attention, such as . BERT instead leverages the self-attention mechanism in the Transformer to unify these two stages. Encoding with self-attention is performed jointly with iterative and bidirectional cross attention. Therefore, many downstream tasks---whether they involve single text or text pairs---can be modeled by simply swapping out the appropriate inputs and outputs.", "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text- varnothing pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.", "During the fine-tuning stage, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. We describe the task-specific details in the corresponding subsections of Section. In-depth task specific design figures are also shown in Appendix.", "Cross-attention BERT is designed such that it does not distinguish between self-attention used within a single sequence and cross-attention used between multiple sequences. Cross-attention between question and passage has been shown to be important for question-answering, where they cross attend question and passage for two times. When fine-tuning a twelve-layer BERT for question answering, the question and passage will cross attend to each other for twelve times given that question and passage are packed into a single input sequence iteratively. Moreover, the parameters for cross-attention are pretrained. The iterative attention also happened in OpenAI GPT, but the second sequence cannot attend to the first sequence due to the unidirectionality constraint.", "The General Language Understanding Evaluation (GLUE) benchmark is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix.", "To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section, and use the final hidden vector C in R^{H} corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights W in R^{K times H}, where K is the number of labels. We compute a standard classification loss with C and W, i.e., log({ rm softmax}(CW^T)).", "We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT_LARGE we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.", "Results are presented in Table. Both BERT_BASE and BERT_LARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT_BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard, BERT_LARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.", "We find that BERT_LARGE significantly outperforms BERT_BASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section.", "The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs. Given a question and a passage from Wikipedia containing the answer, the task is to predict the answer text span in the passage.", "As shown in Figure, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector S in R^H and an end vector E in R^H during fine-tuning. The probability of word i being the start of the answer span is computed as a dot product between T_i and S followed by a softmax over all of the words in the paragraph: . The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as S{ cdot}T_i + E{ cdot}T_j, and the maximum scoring span where j geq i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.", "Table shows top leaderboard entries as well as results from top published systems. The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA befor fine-tuning on SQuAD.", "Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA fine-tuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin.", "The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.", "We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: to the score of the best non-null span s_{i,j} = . We predict a non-null answer when is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.", "The results compared to prior leaderboard entries and top published work are shown in Table, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system.", "The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most plausible continuation among four choices.", "When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice which is normalized with a softmax layer.", "We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table. BERT_LARGE outperforms the authors' baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.", "In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional ablation studies can be found in Appendix.", "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERT_BASE:", "No NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.", "LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.", "We first examine the impact brought by the NSP task. In Table, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \"No NSP\" to \"LTR & No NSP\". The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.", "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right-side context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pre-trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.", "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.", "In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.", "Results on selected GLUE tasks are shown in Table. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters . By contrast, BERT_BASE contains 110M parameters and BERT_LARGE contains 340M parameters.", "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. Both of these prior works used a feature-based approach --- we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.", "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.", "In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task. In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.", "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.", "Results are presented in Table. BERT_LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches.", "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.", "Recently, neural models pretrained on a language modeling task, such as ELMo, OpenAI GPT, and BERT, have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert", "We have seen rapid progress in machine reading compression in recent years with the introduction of large-scale datasets, such as SQuAD, MS MARCO, SearchQA, TriviaQA, and QUASAR-T, and the broad adoption of neural models, such as BiDAF, DrQA, DocumentQA, and QAnet.", "The information retrieval (IR) community has also experienced a flourishing development of neural ranking models, such as DRMM, KNRM, Co-PACRR, and DUET. However, until recently, there were only a few large datasets for passage ranking, with the notable exception of the TREC-CAR. This, at least in part, prevented the neural ranking models from being successful when compared to more classical IR techniques.", "We argue that the same two ingredients that made possible much progress on the reading comprehension task are now available for passage ranking task. Namely, the MS MARCO passage ranking dataset, which contains one million queries from real users and their respective relevant passages annotated by humans, and BERT, a powerful general purpose natural language processing model.", "In this paper, we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state-of-the-art results on the MS MARCO passage re-ranking task.", "A simple question-answering pipeline consists of three main stages. First, a large number (for example, a thousand) of possibly relevant documents to a given question are retrieved from a corpus by a standard mechanism, such as BM25. In the second stage, passage re-ranking, each of these documents is scored and re-ranked by a more computationally-intensive method. Finally, the top ten or fifty of these documents will be the source for the candidate answers by an answer generation module. In this paper, we describe how we implemented the second stage of this pipeline, passage re-ranking.", "The job of the re-ranker is to estimate a score s_i of how relevant a candidate passage d_i is to a query q. We use BERT as our re-ranker. Using the same notation used by, we feed the query as sentence A and the passage text as sentence B. We truncate the query to have at most 64 tokens. We also truncate the passage text such that the concatenation of query, passage, and separator tokens have the maximum length of 512 tokens. We use a BERT_LARGE model as a binary classification model, that is, we use the [CLS] vector as input to a single layer neural network to obtain the probability of the passage being relevant. We compute this probability for each passage independently and obtain the final list of passages by ranking them with respect to these probabilities.", "We start training from a pre-trained BERT model and fine-tune it to our re-ranking task using the cross-entropy loss:", "where J_{pos} is the set of indexes of the relevant passages and J_{neg} is the set of indexes of non-relevant passages in top-1,000 documents retrieved with BM25.", "The training set contains approximately 400M tuples of a query, relevant and non-relevant passages. The development set contains approximately 6,900 queries, each paired with the top 1,000 passages retrieved with BM25 from the MS MARCO corpus. On average, each query has one relevant passage. However, some have no relevant passage because the corpus was initially constructed by retrieving the top-10 passages from the Bing search engine and then annotated. Hence, some of the relevant passages might not be retrieved by BM25.", "An evaluation set with approximately 6,800 queries and their top 1,000 retrieved passages without relevance annotations is also provided.", "We fine-tune the model using a TPU v3-8 with a batch size of 128 (128 sequences * 512 tokens = 65,536 tokens/batch) for 100k iterations, which takes approximately 30 hours. This corresponds to training on 12.8M (100k * 128) query-passage pairs or less than 2% of the full training set. We could not see any improvement in the dev set when training for another 3 days, which equivalent to seeing 50M pairs in total.", "We use ADAM with the initial learning rate set to 3 times 10^{-6}, beta_1 = 0.9, beta_2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers.", "Introduced by, in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section. The corpus consists of all of the English Wikipedia paragraphs, except the abstracts. The released dataset has five predefined folds, and we use the first four as a training set (approximately 2.3M queries), and the remaining as a validation set (approximately 580k queries). The test set is the same one used to evaluate the submissions to TREC-CAR 2017 (approx. 2,254 queries).", "Although TREC-CAR 2017 organizers provide manual annotations for the test set, only the top five passages retrieved by the systems submitted to the competition have manual annotations. This means that true relevant passages are not annotated if they rank low. Hence, we evaluate using the automatic annotations, which provide relevance scores for all possible query-passage pairs.", "We follow the same procedure described for the MS MARCO dataset to fine-tune our models on TREC-CAR. However, there is an important difference. The official pre-trained BERT models were pre-trained on the full Wikipedia, and therefore they have seen, although in an unsupervised way, Wikipedia documents that are used in the test set of TREC-CAR. Thus, to avoid this leak of test data into training, we pre-trained the BERT re-ranker only on the half of Wikipedia used by TREC-CAR's training set.", "For the fine-tuning data, we generate our query-passage pairs by retrieving the top ten passages from the entire TREC-CAR corpus using BM25. This means that we end up with 30M example pairs (3M queries * 10 passages/query) to train our model. We train it for 400k iterations, or 12.8M examples (400k iterations * 32 pairs/batch), which corresponds to only 40% of the training set. Similarly to MS MARCO experiments, we did not see any gain on the dev set by training the models longer.", "We show the main result in Table. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on both of the tasks.", "Training size vs performance: We found that the pretrained models used in this work require few training examples from the end task to achieve a good performance. For example, a BERT_LARGE trained on 100k question-passage pairs (less than 0.3% of the MS MARCO training data) is already 1.4 MRR@10 points better than the previous state-of-the-art, IR-NET.", "We have described a simple adaptation of BERT as a passage re-ranker that has become the state of the art on two different tasks, which are TREC-CAR and MS MARCO. We have made the code to reproduce our experiments publicly available.", "Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.", "Natural language processing (NLP) has been shaken in recent months with the dramatic successes enabled by transfer learning and contextual word embedding models, such as ELMo , ULMFiT , and BERT .", "These models have been primarily explored for general domain text, and, recently, biomedical text with BioBERT . However, clinical narratives (e.g., physician notes) have known differences in linguistic characteristics from both general text and non-clinical biomedical text, motivating the need for specialized clinical BERT models. In this work, we build and publicly release exactly such an embedding model. Furthermore, we demonstrate on several clinical NLP tasks the improvements this system offers over traditional BERT and BioBERT alike. In particular, we make the following contributions: enumerate We train and publicly release BERT-Base and BioBERT-finetuned models trained on both all clinical notes and only discharge summaries. We demonstrate that using clinical specific contextual embeddings improves both upon general domain results and BioBERT results across 2 well established clinical NER tasks and one medical natural language inference task (i2b2 2010 , i2b2 2012 , and MedNLI ). On 2 de-identification (de-ID) tasks, i2b2 2006 and i2b2 2014 , general BERT and BioBERT outperform clinical BERT and we argue that fundamental facets of the de-ID context motivate this lack of performance. enumerate", "Contextual Embeddings in General Traditional word-level vector representations, such as word2vec, GloVe, and fastText, express all possible meanings of a word as a single vector representation and cannot disambiguate the word senses based on the surrounding context. Over the last two years, ELMo and BERT present strong solutions that can provide contextualized word representations. By pre-training on a large text corpus as a language model, ELMo can create a context-sensitive embedding for each word in a given sentence, which will be fed into downstream tasks. Compared to ELMo, BERT is deeper and contains much more parameters, thus possessing greater representation power. More importantly, rather than simply providing word embeddings as features, BERT can be incorporated into a downstream task and gets fine-tuned as an integrated task-specific architecture.", "BERT has, in general, been found to be superior to ELMo and far superior to non-contextual embeddings on a variety of tasks, including those in the clinical domain . For this reason, we only examine BERT here, rather than including ELMo or non-contextual embedding methods.", "Contextual Clinical & Biomedical Embeddings Several works have explored the utility of contextual models in the clinical and biomedical domains. BioBERT trains a BERT model over a corpus of biomedical research articles sourced from PubMed article abstracts and PubMed Central article full texts. They find the specificity offered by biomedical texts translated to improved performance on several biomedical NLP tasks, and fully release their pre-trained BERT model.", "On clinical text, uses a general-domain pretrained ELMo model towards the task of clinical text de-identification, reporting near state-of-the-art performance on the i2b2 2014 task and state of the art performance on several axes of the HIPAA PHI dataset.", "trains an ELMo model over a corpus of mixed clinical discharge summaries, clinical radiology notes and medically oriented wikipedia articles, then demonstrates improved performance on the i2b2 2010 task . They release a pre-trained ELMo model along with their work, enabling further clinical NLP research to work with these powerful contextual embeddings.", ", released in late February 2019, train a clinical note corpus BERT language model and uses complex task-specific models to yield improvements over both traditional embeddings and ELMo embeddings on the i2b2 2010 and 2012 tasks and the SemEval 2014 task 7 and 2015 task 14 tasks, establishing new state-of-the-art results on all four corpora. However, this work neither releases their embeddings for the larger community nor examines the performance opportunities offered by fine-tuning BioBERT with clinical text or by training note-type specific embedding models, as we do.", "In this section, we first describe our clinical text dataset, the details of the BERT training procedure, and finally the specific tasks we examine.", "We use clinical text from the approximately 2 million notes in the MIMIC-III v1.4 database. Details of our text pre-processing procedure can be found in Appendix. Note that while some of our tasks use a small subset of MIMIC notes in their corpora, we do not try to filter these notes out of our BERT pre-training procedure. We expect the bias this induces is negligible given the relative sizes of the two corpora.", "We train two varieties of BERT on MIMIC notes: Clinical BERT, which uses text from all note types, and Discharge Summary BERT, which uses only discharge summaries in an effort to tailor the corpus to downstream tasks (which often largely use discharge summaries).", "Note that we train our clinical BERT instantiations on all notes of the appropriate type(s), without regard for whether or not any individual note appeared in any of the train/test sets for the various tasks we use (two of which use a small subset of MIMIC notes either partially or completely as their backing corpora). We feel this has a negligible impact given the dramatically larger size of the entire MIMIC corpus relative to the various task corpora.", "In this work, we aim to provide the pre-trained embeddings as a community resource, rather than demonstrate technical novelty in the training procedure, and accordingly our BERT training procedure is completely standard. As such, we have relegated specifics of the training procedure to Appendix.", "We trained two BERT models on clinical text: 1) Clinical BERT, initialized from BERT-Base, and 2) Clinical BioBERT, initialized from BioBERT. For all downstream tasks, BERT models were allowed to be fine-tuned, then the output BERT embedding was passed through a single linear layer for classification, either at a per-token level for NER or de-ID tasks or applied to the sentinel \"begin sentence\" token for MedNLI. Note that this is a substantially lower capacity model than, for example, the Bi-LSTM layer used in . This reduced capacity potentially limits performance on downstream tasks, but is in line with our goal of demonstrating the efficacy of clinical-specific embeddings and releasing a pre-trained BERT model for these embeddings. We did not experiment with more complex representations as our goal is not to necessarily surpass state-of-the-art performances on these tasks.", "Computational Cost Pre-processing and training BERT on MIMIC notes took significant computational resources. We estimate that our entire embedding model procedure took roughly 17 - 18 days of computational runtime using a single GeForce GTX TITAN X 12 GB GPU (and significant CPU power and memory for pre-processing tasks). This is not including any time required to download or setup MIMIC or to train any final downstream tasks. 18 days of continuous runtime is a significant investment and may be beyond the reach of some labs or institutions. This is precisely why we believe that releasing our pre-trained model will be useful to the community.", "The Clinical BERT and Clinical BioBERT models were applied to the MedNLI natural language inference task and four i2b2 named entity recognition (NER) tasks, all in IOB format: i2b2 2006 1B de-identification , i2b2 2010 concept extraction , i2b2 2012 entity extraction challenge , i2b2 2014 7A de-identification challenge . Details of the IOB format can be seen in the appendix, section. All task dataset sizes, evaluation metrics, and number of classes are shown in Table.", "Note that our two de-identification (de-ID) datasets present synthetically-masked PHI in their texts---e.g., they replace instances of real names, hospitals, etc., with synthetic, but consistent and realistic, names, hospitals, etc. As a result, they present significantly different text distributions than traditionally de-identified text (such as MIMIC notes) which will instead present sentinel \"PHI\" symbols at locations where PHI was removed.", "In this section, we will first describe quantitative comparisons of the various BERT models on the clinical NLP tasks we considered, and second describe qualitative evaluations of the differences between Clinical- and Bio- BERT.", "Clinical NLP Tasks Full results are shown in Table. On three of the five tasks (MedNLI, i2b2 2010, and i2b2 2012), clinically fine-tuned BioBERT shows improvements over BioBERT or general BERT. Notably, on MedNLI, clinical BERT actually yields a new state of the art, yielding a performance of 82.7% accuracy as compared to the prior state of the art of 73.5% obtained via the InferSent model . However, on our two de-ID tasks, i2b2 2006 and i2b2 2014, clinical BERT offers no improvements over Bio- or general BERT. This is actually not surprising, and is instead, we argue, a direct consequence of the nature of de-ID challenges.", "De-ID challenge data presents a different data distribution than MIMIC text. In MIMIC, PHI is identified and replaced with sentinel PHI markers, whereas in the de-ID task, PHI is masked with synthetic, but realistic PHI. This data drift would be problematic for any embedding model, but will be especially damaging to contextual embedding models like BERT because the underlying sentence structure will have changed: in raw MIMIC, sentences with PHI will universally have a sentinel PHI token. In contrast, in the de-ID corpus, all such sentences will have different synthetic masks, meaning that a canonical, nearly constant sentence structure present during BERT's training will be non-existent at task-time. For these reasons, we think it is sensible that clinical BERT is not successful on the de-ID corpora. Furthermore, this is a good example for the community given how prevalent the assumption is that contextual embedding models trained on task-like corpora will offer dramatic improvements.", "Overall, we feel our results demonstrates the utility of using domain-specific contextual embeddings for non de-ID clinical NLP tasks. Additionally, on one task Discharge Summary BERT offers performance improvements over Clinical BERT, so it may be that adding greater specificity to the underlying corpus is helpful in some cases. We release both models with this work for public use.", "Qualitative Embedding Comparisons Table shows the nearest neighbors for 3 words each from 3 categories under BioBERT and Clinical BERT. These lists suggest that Clinical BERT retains greater cohesion around medical or clinic-operations relevant terms than does BioBERT. For example, the word \"Discharge\" is most closely associated with \"admission,\" \"wave,\" and \"sight\" under BioBERT, yet only the former seems relevant to clinical operations. In contrast, under Clinical BERT, the associated words all are meaningful in a clinical operations context. Limitations & Future Work This work has several notable limitations. First, we do not experiment with any more advanced model architectures atop our embeddings. This likely hurts our performance. Second, MIMIC only contains notes from the intensive care unit of a single healthcare institution (BIDMC). Differences in care practices across institutions are significant, and using notes from multiple institutions could offer significant gains. Lastly, our model shows no improvements for either de-ID task we explored. If our hypothesis is correct as to its cause, a possible solution could entail introducing synthetic de-ID into the source clinical text and using that as the source for de-ID tasks going forward.", "In this work, we pretrain and release clinically oriented BERT models, some trained solely on clinical text, and others fine-tuned atop BioBERT. We find robust evidence that our clinical embeddings are superior to general domain or BioBERT specific embeddings for non de-ID tasks, and that using note-type specific corpora can induce further selective performance benefits. To the best of our knowledge, our work is the first to release clinically trained BERT models. Our hope is that all clinical NLP researchers will be able to benefit from these embeddings without the necessity of the significant computational resources required to train these models over the MIMIC corpus.", "This research was funded in part by grants from the National Institutes of Health (NIH): Harvard Medical School Biomedical Informatics and Data Science Research Training Grant T15LM007092 (Co-PIs: Alexa T. McCray, PhD and Nils Gehlenborg, PhD), National Institute of Mental Health (NIMH) grant P50-MH106933, National Human Genome Research Institute (NHGRI) grant U54-HG007963, and National Science Foundation Graduate Research Fellowship Program (NSF GRFP) under Grant No. 1122374. Additional funding was received from the MIT UROP program.", "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics. }", "Automatic evaluation of natural language generation, for example in machine translation and caption generation, requires comparing candidate sentences to annotated references. The goal is to evaluate semantic equivalence. However, commonly used methods rely on surface-form similarity only. For example, Bleu, the most common machine translation metric, simply counts n-gram overlap between the candidate and the reference. While this provides a simple and general measure, it fails to account for meaning-preserving lexical and compositional diversity.", "In this paper, we introduce BERTScore, a language generation evaluation metric based on pre-trained BERT contextual embeddings. BERTScore computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings.", "BERTScore addresses two common pitfalls in n-gram-based metrics. First, such methods often fail to robustly match paraphrases. For example, given the reference people like foreign cars, Bleu and Meteor incorrectly give a higher score to people like visiting places abroad compared to consumers prefer imported cars. This leads to performance underestimation when semantically-correct phrases are penalized because they differ from the surface form of the reference. In contrast to string matching (e.g., in Bleu) or matching heuristics (e.g., in Meteor), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection. Second, n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes. For example, given a small window of size two, Bleu will only mildly penalize swapping of cause and effect clauses (e.g. A because B instead of B because A), especially when the arguments A and B are long phrases. In contrast, contextualized embeddings are trained to effectively capture distant dependencies and ordering.", "We experiment with BERTScore on machine translation and image captioning tasks using the outputs of 363 systems by correlating BERTScore and related metrics to available human judgments. Our experiments demonstrate that BERTScore correlates highly with human evaluations. In machine translation, BERTScore shows stronger system-level and segment-level correlations with human judgments than existing metrics on multiple common benchmarks and demonstrates strong model selection performance compared to Bleu. We also show that BERTScore is well-correlated with human annotators for image captioning, surpassing Spice, a popular task-specific metric. Finally, we test the robustness of BERTScore on the adversarial paraphrase dataset PAWS, and show that it is more robust to adversarial examples than other metrics. The code for BERTScore is available at https://github.com/Tiiiger/bert_scorehttps://github.com/Tiiiger/bert _score.", "Natural language text generation is commonly evaluated using annotated reference sentences. Given a reference sentence x tokenized to k tokens , a generation evaluation metric is a function . Better metrics have a higher correlation with human judgments. Existing metrics can be broadly categorized into using n-gram matching, edit distance, embedding matching, or learned functions.", "The most commonly used metrics for generation count the number of n-grams that occur in the reference x and candidate x. The higher the n is, the more the metric is able to capture word order, but it also becomes more restrictive and constrained to the exact form of the reference.", "Formally, let S^n_{x} and S^n_{x} be the lists of token n-grams (n in Z_+) in the reference x and candidate x sentences. The number of matched n-grams is is an indicator function. The exact match precision ( exactp_n) and recall ( exactr_n) scores are: eqnarray*", "exactp_n = sum_{w in S^n_{ hat{x}} I[w in S^n_{x}]}{ left lvert S^{n}_{x} right rvert} quad and quad", "exactr_n = sum_{w in S^n_{x} I[w in S^n_{x}]}{ left lvert S^{n}_{x} right rvert}. eqnarray* Several popular metrics build upon one or both of these exact matching scores.", "Bleu The most widely used metric in machine translation is Bleu, which includes three modifications to exactp_n. First, each n-gram in the reference can be matched at most once. Second, the number of exact matches is accumulated for all reference-candidate pairs in the corpus and divided by the total number of n-grams in all candidate sentences. Finally, very short candidates are discouraged using a brevity penalty. Typically, Bleu is computed for multiple values of n (e.g. n=1,2,3,4) and the scores are averaged geometrically. A smoothed variant, SentBleu is computed at the sentence level. In contrast to Bleu, BERTScore is not restricted to maximum n-gram length, but instead relies on contextualized embeddings that are able to capture dependencies of potentially unbounded length.", "Meteor computes exactp_1 and exactr_1 while allowing backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases. For example, running may match run if no exact match is possible. Non-exact matching uses an external stemmer, a synonym lexicon, and a paraphrase table. Meteor 1.5 weighs content and function words differently, and also applies importance weighting to different matching types. The more recent Meteor++ 2.0 further incorporates a learned external paraphrase resource. Because Meteor requires external resources, only five languages are supported with the full feature set, and eleven are partially supported. Similar to Meteor, BERTScore allows relaxed matches, but relies on BERT embeddings that are trained on large amounts of raw text and are currently available for 104 languages. BERTScore also supports importance weighting, which we estimate with simple corpus statistics.", "NIST is a revised version of Bleu that weighs each n-gram differently and uses an alternative brevity penalty.", "DeltaBleu modifies multi-reference Bleu by including human annotated negative reference sentences. chrF compares character n-grams in the reference and candidate sentences. chrF++ extends chrF to include word bigram matching. Rouge is a commonly used metric for summarization evaluation. Rouge-n computes exactr_n (usually n=1,2), while Rouge-L is a variant of exactr_1 with the numerator replaced by the length of the longest common subsequence. CIDEr is an image captioning metric that computes cosine similarity between { rm tf}--{ rm idf} weighted n-grams. We adopt a similar approach to weigh tokens differently. Finally, and combine automatic metrics with human judgments for text generation evaluation.", "Several methods use word edit distance or word error rate, which quantify similarity using the number of edit operations required to get from the candidate to the reference. TER normalizes edit distance by the number of reference words, and ITER adds stem matching and better normalization. PER computes position independent error rate, CDER models block reordering as an edit operation. CharacTer and EED operate on the character level and achieve higher correlation with human judgements on some languages.", "Word embeddings are learned dense token representations. Meant 2.0 uses word embeddings and shallow semantic parses to compute lexical and structural similarity. Yisi-1 is similar to Meant 2.0, but makes the use of semantic parses optional. Both methods use a relatively simple similarity computation, which inspires our approach, including using greedy matching and experimenting with a similar importance weighting to Yisi-1. However, we use contextual embeddings, which capture the specific use of a token in a sentence, and potentially capture sequence information. We do not use external tools to generate linguistic structures, which makes our approach relatively simple and portable to new languages. Instead of greedy matching, WMD, WMDO, and SMS propose to use optimal matching based on earth mover's distance. The tradeoff between greedy and optimal matching was studied by . compute similarity with sentence-level representations. In contrast, our token-level computation allows us to weigh tokens differently according to their importance.", "Various metrics are trained to optimize correlation with human judgments. Beer uses a regression model based on character n-grams and word bigrams. Blend uses regression to combine 29 existing metrics. Ruse combines three pre-trained sentence embedding models. All these methods require costly human judgments as supervision for each dataset, and risk poor generalization to new domains, even within a known language and task. and train a neural model to predict if the input text is human-generated. This approach also has the risk of being optimized to existing data and generalizing poorly to new data. In contrast, the model underlying BERTScore is not optimized for any specific evaluation task.", "Given a reference sentence , we use contextual embeddings to represent the tokens, and compute matching using cosine similarity, optionally weighted with inverse document frequency scores. Figure illustrates the computation.", "We use contextual embeddings to represent the tokens in the input sentences x and x. In contrast to prior word embeddings, contextual embeddings, such as BERT and ELMo, can generate different vector representations for the same word in different sentences depending on the surrounding words, which form the context of the target word. The models used to generate these embeddings are most commonly trained using various language modeling objectives, such as masked word prediction.", "We experiment with different models (Section), using the tokenizer provided with each model. Given a tokenized reference sentence , the embedding model generates a sequence of vectors . Similarly, the tokenized candidate . The main model we use is BERT, which tokenizes the input text into a sequence of word pieces, where unknown words are split into several commonly observed sequences of characters. The representation for each word piece is computed with a Transformer encoder by repeatedly applying self-attention and nonlinear transformations in an alternating fashion. BERT embeddings have been shown to benefit various NLP tasks.", "The vector representation allows for a soft measure of similarity instead of exact-string or heuristic matching. The cosine similarity of a reference token sentref_i and a candidate token senthyp_j is . We use pre-normalized vectors, which reduces this calculation to the inner product vref_i^ top vhyp_j. While this measure considers tokens in isolation, the contextual embeddings contain information from the rest of the sentence.", "The complete score matches each token in x to a token in x to compute recall, and each token in x to a token in x to compute precision. We use greedy matching to maximize the matching similarity score, where each token is matched to the most similar token in the other sentence. We combine precision and recall to compute an F1 measure. For a reference x and candidate x, the recall, precision, and F1 scores are:", "Previous work on similarity measures demonstrated that rare words can be more indicative for sentence similarity than common words. BERTScore enables us to easily incorporate importance weighting. We experiment with inverse document frequency ({ rm idf}) scores computed from the test corpus. Given M reference sentences {x^{(i)} }_{i=1}^M, the { rm idf} score of a word-piece token w is", "where I[ cdot] is an indicator function. We do not use the full { rm tf}-{ rm idf} measure because we process single sentences, where the term frequency ({ rm tf}) is likely 1. For example, recall with { rm idf} weighting is", "Because we use reference sentences to compute { rm idf}, the { rm idf} scores remain the same for all systems evaluated on a specific test set. We apply plus-one smoothing to handle unknown word pieces.", "Because we use pre-normalized vectors, our computed scores have the same numerical range of cosine similarity (between -1 and 1). However, in practice we observe scores in a more limited range, potentially because of the learned geometry of contextual embeddings. While this characteristic does not impact BERTScore's capability to rank text generation systems, it makes the actual score less readable. We address this by rescaling BERTScore with respect to its empirical lower bound b as a baseline. We compute b using Common Crawl monolingual datasets. For each language and contextual embedding model, we create 1M candidate-reference pairs by grouping two random sentences. Because of the random pairing and the corpus diversity, each pair has very low lexical and semantic overlapping. computed on these pairs is around zero.} We compute b by averaging BERTScore computed on these sentence pairs. Equipped with baseline b, we rescale BERTScore linearly. For example, the rescaled value R_BERT of R_BERT is:", "After this operation R_BERT is typically between 0 and 1. We apply the same rescaling procedure for P_BERT and F_BERT. This method does not affect the ranking ability and human correlation of BERTScore, and is intended solely to increase the score readability.", "Contextual Embedding Models We evaluate twelve pre-trained contextual embedding models, including variants of BERT, RoBERTa, XLNet, and XLM. We present the best-performing models in Section. We use the 24-layer model for English tasks, 12-layer model for Chinese tasks, and the 12-layer cased multilingual model for other languages.{https://github.com/huggingface/pytorch-transformers}.} We show the performance of all other models in Appendix. Contextual embedding models generate embedding representations at every layer in the encoder network. Past work has shown that intermediate layers produce more effective representations for semantic tasks. We use the WMT16 dataset as a validation set to select the best layer of each model (Appendix).", "Machine Translation Our main evaluation corpus is the WMT18 metric evaluation dataset, which contains predictions of 149 translation systems across 14 language pairs, gold references, and two types of human judgment scores. Segment-level human judgments assign a score to each reference-candidate pair. System-level human judgments associate each system with a single score based on all pairs in the test set. WMT18 includes translations from English to Czech, German, Estonian, Finnish, Russian, and Turkish, and from the same set of languages to English. We follow the WMT18 standard practice and use absolute Pearson correlation lvert rho rvert and Kendall rank correlation tau to evaluate metric quality, and compute significance with the Williams test for lvert rho rvert and bootstrap re-sampling for tau as suggested by . We compute system-level scores by averaging BERTScore for every reference-candidate pair. We also experiment with hybrid systems by randomly sampling one candidate sentence from one of the available systems for each reference sentence. This enables system-level experiments with a higher number of systems. Human judgments of each hybrid system are created by averaging the WMT18 segment-level human judgments for the corresponding sentences in the sampled data. We compare BERTScores to one canonical metric for each category introduced in Section, and include the comparison with all other participating metrics from WMT18 in Appendix.", "In addition to the standard evaluation, we design model selection experiments. We use 10K hybrid systems super-sampled from WMT18. We randomly select 100 out of 10K hybrid systems, and rank them using the automatic metrics. We repeat this process 100K times. We report the percentage of the metric ranking agreeing with the human ranking on the best system (Hits@1). In Tables-, we include two additional measures to the model selection study: (a) the mean reciprocal rank of the top metric-rated system according to the human ranking, and (b) the difference between the human score of the top human-rated system and that of the top metric-rated system.", "Additionally, we report the same study on the WMT17 and the WMT16 datasests in Appendix. This adds 202 systems to our evaluation.", "We use the human judgments of twelve submission entries from the COCO 2015 Captioning Challenge. Each participating system generates a caption for each image in the COCO validation set, and each image has approximately five reference captions. Following , we compute the Pearson correlation with two system-level metrics: the percentage of captions that are evaluated as better or equal to human captions (M1) and the percentage of captions that are indistinguishable from human captions (M2). We compute BERTScore with multiple references by scoring the candidate with each available reference and returning the highest score. We compare with eight task-agnostic metrics: Bleu, Meteor , Rouge-L, CIDEr, BEER, EED, chrF++, and CharacTER. We also compare with two task-specific metrics: Spice and Leic. Spice is computed using the similarity of scene graphs parsed from the reference and candidate captions. Leic is trained to predict if a caption is written by a human given the image.", "Machine Translation Tables-- show system-level correlation to human judgements, correlations on hybrid systems, and model selection performance. We observe that BERTScore is consistently a top performer. In to-English results, RUSE shows competitive performance. However, RUSE is a supervised method trained on WMT16 and WMT15 human judgment data. In cases where RUSE models were not made available, such as for our from-English experiments, it is not possible to use RUSE without additional data and training. Table shows segment-level correlations. We see that BERTScore exhibits significantly higher performance compared to the other metrics. The large improvement over Bleu stands out, making BERTScore particularly suitable to analyze specific examples, where SentBleu is less reliable. In Appendix, we provide qualitative examples to illustrate the segment-level performance difference between SentBleu and BERTScore. At the segment-level, BERTScore even significantly outperforms RUSE. Overall, we find that applying importance weighting using { rm idf} at times provides small benefit, but in other cases does not help. Understanding better when such importance weighting is likely to help is an important direction for future work, and likely depends on the domain of the text and the available test data. We continue without { rm idf} weighting for the rest of our experiments. While recall R_BERT, precision P_BERT, and F1 F_BERT alternate as the best measure in different setting, F1 F_BERT performs reliably well across all the different settings. Our overall recommendation is therefore to use F1. We present additional results using the full set of 351 systems and evaluation metrics in Tables-- in the appendix, including for experiments with { rm idf} importance weighting, different contextual embedding models, and model selection.", "Table shows correlation results for the COCO Captioning Challenge. BERTScore outperforms all task-agnostic baselines by large margins. Image captioning presents a challenging evaluation scenario, and metrics based on strict n-gram matching, including Bleu and Rouge, show weak correlations with human judgments. { rm idf} importance weighting shows significant benefit for this task, suggesting people attribute higher importance to content words. Finally, Leic, a trained metric that takes images as additional inputs and is optimized specifically for the COCO data and this set of systems, outperforms all other methods.", "Despite the use of a large pre-trained model, computing BERTScore is relatively fast. We are able to process 192.5 candidate-reference pairs/second using a GTX-1080Ti GPU. The complete WMT18 en-de test set, which includes 2,998 sentences, takes 15.6sec to process, compared to 5.4sec with SacreBLEU, a common Bleu implementation. Given the sizes of commonly used test and validation sets, the increase in processing time is relatively marginal, and BERTScore is a good fit for using during validation (e.g., for stopping) and testing, especially when compared to the time costs of other development stages.", "We test the robustness of BERTScore using adversarial paraphrase classification. We use the Quora Question Pair corpus and the adversarial paraphrases from the Paraphrase Adversaries from Word Scrambling dataset. Both datasets contain pairs of sentences labeled to indicate whether they are paraphrases or not. Positive examples in QQP are real duplicate questions, while negative examples are related, but different questions. Sentence pairs in PAWS are generated through word swapping. For example, in PAWS, Flights from New York to Florida may be changed to Flights from Florida to New York and a good classifier should identify that these two sentences are not paraphrases. PAWS includes two parts: PAWSQQP, which is based on the QQP data, and PAWSWiki. We use the PAWSQQP development set which contains 667 sentences. For the automatic metrics, we use no paraphrase detection training data. We expect that pairs with higher scores are more likely to be paraphrases. To evaluate the automatic metrics on QQA, we use the first 5,000 sentences in the training set instead of the the test set because the test labels are not available. We treat the first sentence as the reference and the second sentence as the candidate.", "Table reports the area under ROC curve (AUC) for existing models and automatic metrics. We observe that supervised classifiers trained on QQP perform worse than random guess on PAWSQQP, which shows these models predict the adversarial examples are more likely to be paraphrases. When adversarial examples are provided in training, state-of-the-art models like DIIN and fine-tuned BERT are able to identify the adversarial examples but their performance still decreases significantly from their performance on QQP. Most metrics have decent performance on QQP, but show a significant performance drop on PAWSQQP, almost down to chance performance. This suggests these metrics fail to to distinguish the harder adversarial examples. In contrast, the performance of BERTScore drops only slightly, showing more robustness than the other metrics.", "We propose BERTScore, a new metric for evaluating generated text against gold standard references. BERTScore is purposely designed to be simple, task agnostic, and easy to use. Our analysis illustrates how BERTScore resolves some of the limitations of commonly used metrics, especially on challenging adversarial examples. We conduct extensive experiments with various configuration choices for BERTScore, including the contextual embedding model used and the use of importance weighting. Overall, our extensive experiments, including the ones in the appendix, show that BERTScore achieves better correlation than common metrics, and is effective for model selection. However, there is no one configuration of BERTScore that clearly outperforms all others. While the differences between the top configurations are often small, it is important for the user to be aware of the different trade-offs, and consider the domain and languages when selecting the exact configuration to use. In general, for machine translation evaluation, we suggest using F_BERT, which we find the most reliable. For evaluating text generation in English, we recommend using the 24-layer . For non-English language, the multilingual is a suitable choice although BERTScore computed with this model has less stable performance on low-resource languages. We report the optimal hyperparameter for all models we experimented with in Appendix", "Briefly following our initial preprint publication, published a concurrently developed method related to ours, but with a focus on integrating contextual word embeddings with earth mover's distance rather than our simple matching process. They also propose various improvements compared to our use of contextualized embeddings. We study these improvements in Appendix and show that integrating them into BERTScore makes it equivalent or better than the EMD-based approach. Largely though, the effect of the different improvements on BERTScore is more modest compared to their method. Shortly after our initial publication, YiSi-1 was updated to use BERT embeddings, showing improved performance. This further corroborates our findings. Other recent related work includes training a model on top of BERT to maximize the correlation with human judgments and evaluating generation with a BERT model fine-tuned on paraphrasing. More recent work shows the potential of using BERTScore for training a summarization system and for domain-specific evaluation using SciBERT to evaluate abstractive text summarization.", "In future work, we look forward to designing new task-specific metrics that use BERTScore as a subroutine and accommodate task-specific needs, similar to how suggests to use semantic similarity for machine translation training. Because BERTScore is fully differentiable, it also can be incorporated into a training procedure to compute a learning loss that reduces the mismatch between optimization and evaluation objectives.", "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.", "Pre-trained sentence encoders such as ELMo and BERT have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings and discrete pipelines as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics.", "A wave of recent work has begun to \"probe\" state-of-the-art models to understand whether they are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in order to reverse-engineer the types of abstractions the model may or may not be representing . Parallel efforts inspect the structure of the network directly, to assess whether there exist localizable regions associated with distinct types of linguistic decisions. Such work has produced evidence that deep language models can encode a range of syntactic and semantic information , and that more complex structures are represented hierarchically in the higher layers of the model .", "We build on this latter line of work, focusing on the BERT model , and use a suite of probing tasks derived from the traditional NLP pipeline to quantify where specific types of linguistic information are encoded. Building on observations that lower layers of a language model encode more local syntax while higher layers capture more complex semantics, we present two novel contributions. First, we present an analysis that spans the common components of a traditional NLP pipeline. We show that the order in which specific abstractions are encoded reflects the traditional hierarchy of these tasks. Second, we qualitatively analyze how individual sentences are processed by the BERT network, layer-by-layer. We show that while the pipeline order holds in aggregate, the model can allow individual decisions to depend on each other in arbitrary ways, deferring ambiguous decisions or revising incorrect ones based on higher-level information.", "Edge Probing. Our experiments are based on the \"edge probing\" approach of , which aims to measure how well information about linguistic structure can be extracted from a pre-trained encoder. Edge probing decomposes structured-prediction tasks into a common format, where a probing classifier receives spans s_1 = [i_1, j_1) and (optionally) s_2 = [i_2, j_2) and must predict a label such as a constituent or relation type._2 is not used. For POS, s_1 = [i,i+1) is a single token.} The probing classifier has access only to the per-token contextual vectors within the target spans, and so must rely on the encoder to provide information about the relation between these spans and their role in the sentence.", "We use eight labeling tasks from the edge probing suite: part-of-speech (POS), constituents (Consts.), dependencies (Deps.), entities, semantic role labeling (SRL), coreference (Coref.), semantic proto-roles , and relation classification (SemEval). These tasks are derived from standard benchmark datasets, and evaluated with a common metric--micro-averaged F1--to facilitate comparison across tasks.", "BERT. The BERT model has shown state-of-the-art performance on many tasks, and its deep Transformer architecture is typical of many recent models . We focus on the stock BERT models (base and large, uncased), which are trained with a multi-task objective (masked language modeling and next-sentence prediction) over a 3.3B word English corpus. Since we want to understand how the network represents language as a result of pretraining, we follow (departing from standard BERT usage) and freeze the encoder weights. This prevents the encoder from re-arranging its internal representations to better suit the probing task.", "Given input tokens T = [t_0, t_1, ldots, t_n], a deep encoder produces a set of layer activations are the activation vectors of the ell^{th} encoder layer and H^{(0)} corresponds to the non-contextual word(piece) embeddings. We use a weighted sum across layers ( S) to pool these into a single set of per-token representation vectors , and train a probing classifier P_{ tau} for each task using the architecture and procedure of .", "Limitations This work is intended to be exploratory. We focus on one particular encoder--BERT--to explore how information can be organized in a deep language model, and further work is required to determine to what extent the trends hold in general. Furthermore, our work carries the limitations of all inspection-based probing: the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used. For this reason, we emphasize the importance of combining structural analysis with behavioral studies (as discussed in S) to provide a more complete picture of what information these models encode and how that information affects performance on downstream tasks.", "We define two complementary metrics. The first, scalar mixing weights ( S) tell us which layers, in combination, are most relevant when a probing classifier has access to the whole BERT model. The second, cumulative scoring ( S) tells us how much higher we can score on a probing task with the introduction of each layer. These metrics provide complementary views on what is happening inside the model. Mixing weights are learned solely from the training data--they tell us which layers the probing model finds most useful. In contrast, cumulative scoring is derived entirely from an evaluation set, and tell us how many layers are needed for a correct prediction.", "To pool across layers, we use the scalar mixing technique introduced by the ELMo model. Following Equation~(1) of , for each task we introduce scalar parameters gamma_{ tau} and . We learn these weights jointly with the probing classifier {P}_{ tau}, in order to allow it to extract information from the many layers of an encoder without adding a large number of parameters. After the probing model is trained, we extract the learned coefficients in order to estimate the contribution of different layers to that particular task. We interpret higher weights as evidence that the corresponding layer contains more information related to that particular task.", "This reflects the average layer attended to for each task; intuitively, we can interpret a higher value to mean that the information needed for that task is captured by higher layers.", "We would like to estimate at which layer in the encoder a target can be correctly predicted. Mixing weights cannot tell us this directly, because they are learned as parameters and do not correspond to a distribution over data. A naive classifier at a single layer cannot either, because information about a particular span may be spread out across several layers, and as observed in the encoder may choose to discard information at higher layers.", "To address this, we train a series of classifiers {{P}^{( ell)}_{ tau} }_ ell which use scalar mixing (Eq.) to attend to layer ell as well as all previous layers. {P}^{(0)}_{ tau} corresponds to a non-contextual baseline that uses only a bag of word(piece) embeddings, while {P}^{(L)}_{ tau} = {P}_{ tau} corresponds to probing all layers of the BERT model.", "These classifiers are cumulative, in the sense that P^{( ell+1)}_{ tau} has a similar number of parameters but with access to strictly more information than P^{( ell)}_{ tau}, and we see intuitively that performance (F1 score) generally increases as more layers are added. We can then compute a differential score Delta^{( ell)}_{ tau}, which measures how much better we do on the probing task if we observe one additional encoder layer ell:", "Expected Layer. Again, we compute a (pseudo) expectation over the differential scores as a summary statistic. To focus on the behavior of the contextual encoder layers, we omit the contribution of both the \"trivial\" examples resolved at layer 0, as well as the remaining headroom from the full model. Let:", "This can be thought of as, approximately, the expected layer at which the probing model correctly labels an example, assuming that example is resolved at some layer ell ge 1 of the encoder.", "Figure reports summary statistics and absolute F1 scores, and Figure reports per-layer metrics. Both show results on the 24-layer BERT-large model. We also report to estimate how non-uniform each statistic () is for each task.", "Linguistic Patterns. We observe a consistent trend across both of our metrics, with the tasks encoded in a natural progression: POS tags processed earliest, followed by constituents, dependencies, semantic roles, and coreference. That is, it appears that basic syntactic information appears earlier in the network, while high-level semantic information appears at higher layers. We note that this finding is consistent with initial observations by , which found that constituents are represented earlier than coreference.", "In addition, we observe that in general, syntactic information is more localizable, with weights related to syntactic tasks tending to be concentrated on a few layers (high K(s) and K( Delta)), while information related to semantic tasks is generally spread across the entire network. For example, we find that for semantic relations and proto-roles (SPR), the mixing weights are close to uniform, and that nontrivial examples for these tasks are resolved gradually across nearly all layers. For entity labeling many examples are resolved in layer 1, but with a long tail thereafter, and only a weak concentration of mixing weights in high layers. Further study is needed to determine whether this is because BERT has difficulty representing the correct abstraction for these tasks, or because semantic information is inherently harder to localize.", "Comparison of Metrics. For many tasks, we find that the differential scores are highest in the first few layers of the model (layers 1-7 for BERT-large), i.e. most examples can be correctly classified very early on. We attribute this to the availability of heuristic shortcuts: while challenging examples may not be resolved until much later, many cases can be guessed from shallow statistics. Conversely, we observe that the learned mixing weights are concentrated much later, layers 9-20 for BERT-large. We observe--particularly when weights are highly concentrated--that the highest weights are found on or just after the highest layers which give an improvement Delta^{( ell)}_{ tau} in F1 score for that task.", "This helps explain the observations on the semantic relations and SPR tasks: cumulative scoring shows continued improvement up to the highest layers of the model, while the lack of concentration in the mixing weights suggest that the BERT encoder does not expose a localized set of features that encode these more semantic phenomena. Similarly for entity types, we see continued improvements in the higher layers -- perhaps related to fine-grained semantic distinctions like \"Organization\" (ORG) vs. \"Geopolitical Entity\" (GPE) -- while the low value for the expected layer reflects that many examples require only limited context to resolve.", "Comparison of Encoders. We observe the same general ordering on the 12-layer BERT-base model (Figure). In particular, there appears to be a \"stretching effect\", where the representations for a given task tend to concentrate at the same layers relative to the top of the model; this is illustrated side-by-side in Figure.", "We explore, qualitatively, how beliefs about the structure of individual sentences develop over the layers of the BERT network. The OntoNotes development set contains annotations for five of our probing tasks: POS, constituents, entities, SRL, and coreference. We compile the predictions of the per-layer classifiers {P}^{( ell)}_{ tau} for each task. Because many annotations are uninteresting -- for example, 89% of part-of-speech tags are correct at layer 0 -- we use a heuristic to identify ambiguous sentences to visualize._1, s_2, label) where the highest scoring label has an average score , and look at sentences with more than one such edge.} Figure shows two selected examples, and more are presented in Appendix.", "We find that while the pipeline order holds on average (Figure), for individual examples the model is free to and often does choose a different order. In the first example, the model originally (incorrectly) assumes that \"Toronto\" refers to the city, tagging it as a GPE. However, after resolving the semantic role -- determining that \"Toronto\" is the thing getting \"smoked\" (ARG1) -- the entity-typing decision is revised in favor of ORG (i.e. the sports team). In the second example, the model initially tags \"today\" as a common noun, date, and temporal modifier (ARGM-TMP). However, this phrase is ambiguous, and it later reinterprets \"china today\" as a proper noun (i.e. the TV network) and updates its beliefs about the entity type (to ORG), followed by the semantic role (reinterpreting it as the agent ARG0).", "We employ the edge probing task suite to explore how the different layers of the BERT network can resolve syntactic and semantic structure within a sentence. We present two complementary measurements: scalar mixing weights, learned from a training corpus, and cumulative scoring, measured on an evaluation set, and show that a consistent ordering emerges. We find that while this traditional pipeline order holds in the aggregate, on individual examples the network can resolve out-of-order, using high-level information like predicate-argument relations to help disambiguate low-level decisions like part-of-speech. This provides new evidence corroborating that deep language models can represent the types of syntactic and semantic abstractions traditionally believed necessary for language processing, and moreover that they can model complex interactions between different levels of hierarchical information.", "In this paper, we show that Multilingual BERT (M-Bert), released by as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-Bert does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.", "Deep, contextualized language models provide powerful, general-purpose linguistic representations that have enabled significant advances among a wide range of natural language processing tasks . These models can be pre-trained on large corpora of readily available unannotated text, and then fine-tuned for specific tasks on smaller amounts of supervised data, relying on the induced language model structure to facilitate generalization beyond the annotations. Previous work on model probing has shown that these representations are able to encode, among other things, syntactic and named entity information, but they have heretofore focused on what models trained on English capture about English . In this paper, we empirically investigate the degree to which these representations generalize across languages. We explore this question using Multilingual BERT (henceforth, M-Bert), released by as a single language model pre-trained on the concatenation of monolingual Wikipedia corpora from 104 languages. M-Bert is particularly well suited to this probing study because it enables a very straightforward approach to zero-shot cross-lingual model transfer: we fine-tune the model using task-specific supervised training data from one language, and evaluate that task in a different language, thus allowing us to observe the ways in which the model generalizes information across languages.", "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts---thus having zero lexical overlap---indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "Like the original English BERT model (henceforth, En-Bert), M-Bert is a 12 layer transformer , but instead of being trained only on monolingual English data with an English-derived vocabulary, it is trained on the Wikipedia pages of 104 languages with a shared word piece vocabulary. It does not use any marker denoting the input language, and does not have any explicit mechanism to encourage translation-equivalent pairs to have similar representations.", "For ner and pos, we use the same sequence tagging architecture as . We tokenize the input sentence, feed it to Bert, get the last layer's activations, and pass them through a final layer to make the tag predictions. The whole model is then fine-tuned to minimize the cross entropy loss for the task. When tokenization splits words into multiple pieces, we take the prediction for the first piece as the prediction for the word.", "We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table shows M-Bert zero-shot performance on all language pairs in the CoNLL data.", "We perform pos experiments using Universal Dependencies (UD) data for 41 languages. We use the evaluation sets from . Table shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over 80% accuracy for all pairs.", "Because M-Bert uses a single, multilingual vocabulary, one form of cross-lingual transfer occurs when word pieces present during fine-tuning also appear in the evaluation languages. In this section, we present experiments probing M-Bert's dependence on this superficial form of generalization: How much does transferability depend on lexical overlap? And is transfer possible to languages written in different scripts (no overlap)?", "If M-Bert's ability to generalize were mostly due to vocabulary memorization, we would expect zero-shot performance on ner to be highly dependent on word piece overlap, since entities are often similar across languages. To measure this effect, we compute E_train and E_eval, the sets of word pieces used in entities in the training and evaluation datasets, respectively, and define overlap as the fraction of common word pieces used in the entities: .", "Figure plots ner F1 score versus entity overlap for zero-shot transfer between every language pair in an in-house dataset of 16 languages, for both M-Bert and En-Bert. We can see that performance using En-Bert depends directly on word piece overlap: the ability to transfer deteriorates as word piece overlap diminishes, and F1 scores are near zero for languages written in different scripts. M-Bert's performance, on the other hand, is flat for a wide range of overlaps, and even for language pairs with almost no lexical overlap, scores vary between 40% and 70%, showing that M-Bert's pretraining on multiple languages has enabled a representational capacity deeper than simple vocabulary memorization.", "To further verify that En-Bert's inability to generalize is due to its lack of a multilingual representation and not an inability of its English-specific word piece vocabulary to represent data in other languages, we evaluate on non-cross-lingual ner and see that it performs comparably to a previous state of the art model (see Table ).", "M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table shows a sample of pos results for transfer across scripts.", "Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.", "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.", "In the previous section, we showed that M-Bert's ability to generalize cannot be attributed solely to vocabulary memorization, and that it must be learning a deeper multilingual representation. In this section, we present probing experiments that investigate the nature of that representation: How does typological similarity affect M-Bert's ability to generalize? Can M-Bert generalize from monolingual inputs to code-switching text? Can the model generalize to transliterated text without transliterated language model pretraining?", "Following , we compare languages on a subset of the WALS features relevant to grammatical ordering. Figure plots pos zero-shot accuracy against the number of common WALS features. As expected, performance improves with similarity, showing that it is easier for M-Bert to map linguistic structures when they are more similar, although it still does a decent job for low similarity languages when compared to En-Bert.", "} Table shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "Code-switching (CS)---the mixing of multiple languages within a single utterance---and transliteration---writing that is not in the language's standard script---present unique test cases for M-Bert, which is pre-trained on monolingual, standard-script corpora. Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would benefit to an even larger degree from a shared multilingual representation. Likewise, generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-Bert was not pre-trained on text that looks like the target.", "We test M-Bert on the CS Hindi/English UD corpus from , which provides texts in two formats: transliterated, where Hindi words are written in Latin script, and corrected, where annotators have converted them back to Devanagari script. Table shows the results for models fine-tuned using a combination of monolingual Hindi and English, and using the CS training set (both fine-tuning on the script-corrected version of the corpus as well as the transliterated version).", "For script-corrected inputs, i.e., when Hindi is written in Devanagari, M-Bert's performance when trained only on monolingual corpora is comparable to performance when training on code-switched data, and it is likely that some of the remaining difference is due to domain mismatch. This provides further evidence that M-Bert uses a representation that is able to incorporate information from multiple languages.", "However, M-Bert is not able to effectively transfer to a transliterated target, suggesting that it is the language model pre-training on a particular language that allows transfer to that language. M-Bert is outperformed by previous work in both the monolingual-only and code-switched supervision scenarios. Neither nor use contextualized word embeddings, but both incorporate explicit transliteration signals into their approaches.", "In this section, we study the structure of M-Bert's feature space. If it is multilingual, then the transformation mapping between the same sentence in 2 languages should not depend on the sentence itself, just on the language pair.", "We sample 5000 pairs of sentences from WMT16 and feed each sentence (separately) to M-Bert with no fine-tuning. We then extract the hidden feature activations at each layer for each of the sentences, and average the representations for the input tokens except [cls] and [sep], to get a vector for each sentence, at each layer l, v_lang^{(l)}. For each pair of sentences, e.g. , we compute the vector pointing from one to the other and average it over all pairs: is the number of pairs. Finally, we translate each sentence, v_{en_i}^{(l)}, by , find the closest German sentence vector, and measure the fraction of times the nearest neighbour is the correct pair, which we call the \"nearest neighbor accuracy\".", "In Figure , we plot the nearest neighbor accuracy for en-de (solid line). It achieves over 50% accuracy for all but the bottom layers, which seems to imply that the hidden representations, although separated in space, share a common subspace that represents useful linguistic information, in a language-agnostic way. Similar curves are obtained for en-ru, and ur-hi (in-house dataset), showing this works for multiple languages.", "As to the reason why the accuracy goes down in the last few layers, one possible explanation is that since the model was pre-trained for language modeling, it might need more language-specific information to correctly predict the missing word.", "In this work, we showed that M-Bert's robust, often surprising, ability to generalize cross-lingually is underpinned by a multilingual representation, without being explicitly trained for it. The model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by or .", "As to why M-Bert generalizes across languages, we hypothesize that having word pieces used in all languages (numbers, URLs, etc) which have to be mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close to a shared space.", "It is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.", "We would like to thank Mark Omernick, Livio Baldini Soares, Emily Pitler, Jason Riesa, and Slav Petrov for the valuable discussions and feedback.", "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.", "Large pre-trained language models achieve very high accuracy when fine-tuned on supervised tasks , but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn?", "Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences or examining the internal vector representations of the model through methods such as probing classifiers . Complementary to these approaches, we study the attention maps of a pre-trained model. Attention has been a highly successful neural network component. It is naturally interpretable because an attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word. Our analysis focuses on the 144 attention heads in BERT , a large pre-trained Transformer network that has demonstrated excellent performance on many tasks.", "We first explore generally how BERT's attention heads behave. We find that there are common patterns in their behavior, such as attending to fixed positional offsets or attending broadly over the whole sentence. A surprisingly large amount of BERT's attention focuses on the deliminator token [SEP], which we argue is used by the model as a sort of no-op. Generally we find that attention heads in the same layer tend to behave similarly.", "We next probe each attention head for linguistic phenomena. In particular, we treat each head as a simple no-training-required classifier that, given a word as input, outputs the most-attended-to other word. We then evaluate the ability of the heads to classify various syntactic relations. While no single head performs well at many relations, we find that particular heads correspond remarkably well to particular relations. For example, we find heads that find direct objects of verbs, determiners of nouns, objects of prepositions, and objects of possessive pronouns with >75% accuracy. We perform a similar analysis for coreference resolution, also finding a BERT head that performs quite well. These results are intriguing because the behavior of the attention heads emerges purely from self-supervised training on unlabeled data, without explicit supervision for syntax or coreference.", "Our findings show that particular heads specialize to specific aspects of syntax. To get a more overall measure of the attention heads' syntactic ability, we propose an attention-based probing classifier that takes attention maps as input. The classifier achieves 77 UAS at dependency parsing, showing BERT's attention captures a substantial amount about syntax. Several recent works have proposed incorporating syntactic information to improve attention . Our work suggests that to an extent this kind of syntax-aware attention already exists in BERT, which may be one of the reason for its success.", "Although our analysis methods are applicable to any model that uses an attention mechanism, in this paper we analyze BERT , a large Transformer network. Transformers consist of multiple layers where each layer contains multiple attention heads. An attention head takes as input a sequence of vectors h = [h_1, ..., h_n] corresponding to the n tokens of the input sentence. Each vector h_i is transformed into query, key, and value vectors q_i, k_i, v_i through separate linear transformations. The head computes attention weights alpha between all pairs of words as softmax-normalized dot products between the query and key vectors. The output o of the attention head is a weighted sum of the value vectors.", "alpha_{ij = exp{(q_i^T k_j)}{ sum_{l=1}^n (q_i^T k_l)} quadaa o_i = sum_{j=1}^n alpha_{ij} v_j } Attention weights can be viewed as governing how \"important\" every other token is when producing the next representation for the current token.", "BERT is pre-trained on 3.3 billion tokens of English text to perform two tasks. In the \"masked language modeling\" task, the model predicts the identities of words that have been masked-out of the input text. In the \"next sentence prediction\" task, the model predicts whether the second half of the input follows the first half of the input in the corpus, or is a random paragraph. Further training the model on supervised data results in impressive performance across a variety of tasks ranging from sentiment analysis to question answering. An important detail of BERT is the preprocessing used for the input text. A special token [CLS] is added to the beginning of the text and another token [SEP] is added to the end. If the input consists of multiple separate texts (e.g., a reading comprehension example consists of a separate question and context), [SEP] tokens are also used to separate them. As we show in the next section, these special tokens play an important role in BERT's attention. We use the \"base\" sized BERT model, which has 12 layers containing 12 attention heads each. We use <layer>-<head _number> to denote a particular attention head.", "Before looking at specific linguistic phenomena, we first perform an analysis of surface-level patterns in how BERT's attention heads behave. Examples of heads exhibiting these patterns are shown in Figure.", "Setup We extract the attention maps from BERT-base over 1000 random Wikipedia segments. We follow the setup used for pre-training BERT where each segment consists of at most 128 tokens corresponding to two consecutive paragraphs of Wikipedia (although we do not mask out input words or as in BERT's training). The input presented to the model is [CLS]<paragraph-1>[SEP]<paragraph-2>[SEP].", "First, we compute how often BERT's attention heads attend to the current token, the previous token, or the next token. We find that most heads put little attention on the current token. However, there are heads that specialize to attending heavily on the next or previous token, especially in earlier layers of the network. In particular four attention heads (in layers 2, 4, 7, and 8) on average put >50% of their attention on the previous token and five attention heads (in layers 1, 2, 2, 3, and 6) put >50% of their attention on the next token.", "Interestingly, we found that a substantial amount of BERT's attention focuses on a few tokens (see Figure). For example, over half of BERT's attention in layers 6-10 focuses on [SEP]. To put this in context, since most of our segments are 128 tokens long, the average attention for a token occurring twice in a segments like [SEP] would normally be around 1/64. [SEP] and [CLS] are guaranteed to be present and are never masked out, while periods and commas are the most common tokens in the data excluding \"the,\" which might be why the model treats these tokens differently. A similar pattern occurs for the uncased BERT model, suggesting there is a systematic reason for the attention to special tokens rather than it being an artifact of stochastic training.", "One possible explanation is that [SEP] is used to aggregate segment-level information which can then be read by other heads. However, further analysis makes us doubtful this is the case. If this explanation were true, we would expect attention heads processing [SEP] to attend broadly over the whole segment to build up these representations. However, they instead almost entirely (more than 90%; see bottom of Figure) attend to themselves and the other [SEP] token. Furthermore, qualitative analysis (see Figure) shows that heads with specific functions attend to [SEP] when the function is not called for. For example, in head 8-10 direct objects attend to their verbs. For this head, non-nouns mostly attend to [SEP]. Therefore, we speculate that attention over these special tokens might be used as a sort of \"no-op\" when the attention head's function is not applicable.", "To further investigate this hypothesis, we apply gradient-based measures of feature importance . In particular, we compute the magnitude of the gradient of the loss from BERT's masked language modeling task with respect to each attention weight. Intuitively, this value measures how much changing the attention to a token will change BERT's outputs. Results are shown in Figure. Starting in layer 5 -- the same layer where attention to [SEP] becomes high -- the gradients for attention to [SEP] become very small. This indicates that attending more or less to [SEP] does not substantially change BERT's outputs, supporting the theory that attention to [SEP] is used as a no-op for attention heads.", "Lastly, we measure whether attention heads focus on a few words or attend broadly over many words. To do this, we compute the average entropy of each head's attention distribution (see Figure). We find that some attention heads, especially in lower layers, have very broad attention. These high-entropy attention heads typically spend at most 10% of their attention mass on any single word. The output of these heads is roughly a bag-of-vectors representation of the sentence.", "We also measured entropies for all attention heads from only the [CLS] token. While the average entropies from [CLS] for most layers are very close to the ones shown in Figure, the last layer has a high entropy from [CLS] of 3.89 nats, indicating very broad attention. This finding makes sense given that the representation for the [CLS] token is used as input for the \"next sentence prediction\" task during pre-training, so it attends broadly to aggregate a representation for the whole input in the last layer.", "Next, we investigate individual attention heads to probe what aspects of language they have learned. In particular, we evaluate attention heads on labeled datasets for tasks like dependency parsing. An overview of our results is shown in Figure.", "We wish to evaluate attention heads at word-level tasks, but BERT uses byte-pair tokenization , which means some words ( sim8% in our data) are split up into multiple tokens. We therefore convert token-token attention maps to word-word attention maps. For attention to a split-up word, we sum up the attention weights over its tokens. For attention from a split-up word, we take the mean of the attention weights over its tokens. These transformations preserve the property that the attention from each word sums to 1. For a given attention head and word, we take whichever other word receives the most attention weight as that model's prediction", "Setup. We extract attention maps from BERT on the Wall Street Journal portion of the Penn Treebank annotated with Stanford Dependencies. We evaluate both \"directions\" of prediction for each attention head: the head word attending to the dependent and the dependent attending to the head word. Some dependency relations are simpler to predict than others: for example a noun's determiner is often the immediately preceding word. Therefore as a point of comparison, we show predictions from a simple fixed-offset baseline. For example, a fixed offset of -2 means the word two positions to the left of the dependent is always considered to be the head.", "Results Table shows that there is no single attention head that does well at syntax \"overall\"; the best head gets 34.5 UAS, which is not much better than the right-branching baseline, which gets 26.3 UAS. This finding is similar to the one reported by , who also evaluate individual attention heads for syntax.", "However, we do find that certain attention heads specialize to specific dependency relations, sometimes achieving high accuracy and substantially outperforming the fixed-offset baseline. We find that for all relations in Table except pobj, the dependent attends to the head word rather than the other way around, likely because each dependent has exactly one head but heads have multiple dependents. We also note heads can disagree with standard annotation conventions while still performing syntactic behavior. For example, head 7-6 marks 's as the dependent for the poss relation, while gold-standard labels mark the complement of an 's as the dependent (the accuracy in Table counts 's as correct). Such disagreements highlight how these syntactic behaviors in BERT are learned as a by-product of self-supervised training, not by copying a human design.", "Figure shows some examples of the attention behavior. While the similarity between machine-learned attention weights and human-defined syntactic relations are striking, we note these are relations for which attention heads do particularly well on. There are many relations for which BERT only slightly improves over the simple baseline, so we would not say individual attention heads capture dependency structure as a whole. We think it would be interesting future work to extend our analysis to see if the relations well-captured by attention are similar or different for other languages.", "Having shown BERT attention heads reflect certain aspects of syntax, we now explore using attention heads for the more challenging semantic task of coreference resolution. Coreference links are usually longer than syntactic dependencies and state-of-the-art systems generally perform much worse at coreference compared to parsing.", "Setup We evaluate the attention heads on coreference resolution using the CoNLL-2012 dataset . In particular, we compute antecedent selection accuracy: what percent of the time does the head word of a coreferent mention most attend to the head of one of that mention's antecedents. We compare against three baselines for selecting an antecedent: itemize sep0em Picking the nearest other mention. Picking the nearest other mention with the same head word as the current mention. A simple rule-based system inspired by . It proceeds through 4 sieves: (1) full string match, (2) head word match, (3) number/gender/person match, (4) all other mentions. The nearest mention satisfying the earliest sieve is returned. itemize We also show the performance of a recent neural coreference system from .", "Results Results are shown in Table. We find that one of BERT's attention heads achieves decent coreference resolution performance, improving by over 10 accuracy points on the string-matching baseline and performing close to the rule-based system. It is particularly good with nominal mentions, perhaps because it is capable of fuzzy matching between synonyms as seen in the bottom right of Figure.", "Since individual attention heads specialize to particular aspects of syntax, the model's overall \"knowledge\" about syntax is distributed across multiple attention heads. We now measure this overall ability by proposing a novel family of attention-based probing classifiers and applying them to dependency parsing. For these classifiers we treat the BERT attention outputs as fixed, i.e., we do not back-propagate into BERT and only train a small number of parameters.", "The probing classifiers are basically graph-based dependency parsers. Given an input word, the classifier produces a probability distribution over other words in the sentence indicating how likely each other word is to be the syntactic head of the current one.", "p(i|j) propto exp{ bigg( sum_{k=1^{n} w_k alpha^k_{ij} + u_k alpha^k_{ji} bigg)} } where p(i|j) is the probability of word i being word j's syntactic head, alpha^k_{ij} is the attention weight from word i to word j produced by head k, and n is the number of attention heads. We include both directions of attention: candidate head to dependent as well as dependent to candidate head. The weight vectors w and u are trained using standard supervised learning on the train set.", "Attention-and-Words Probe Given our finding that heads specialize to particular syntactic relations, we believe probing classifiers should benefit from having information about the input words. In particular, we build a model that sets the weights of the attention heads based on the GloVe embeddings for the input words. Intuitively, if the dependent and candidate head are \"the\" and \"cat,\" the probing classifier should learn to assign most of the weight to the head 8-11, which achieves excellent performance at the determiner relation. The attention-and-words probing classifier assigns the probability of word i being word j's head as", "&U_{k,:}(v_i oplus v_j) alpha^k_{ji} bigg) } Where v denotes GloVe embeddings and oplus denotes concatenation. The GloVe embeddings are held fixed in training, so only the two weight matrices W and U are learned. The dot product W_{k,:}(v_i oplus v_j) produces a word-sensitive weight for the particular attention head.", "Results We evaluate our methods on the Penn Treebank dev set annotated with Stanford dependencies. We compare against three baselines: itemize sep0em A right-branching baseline that always predicts the head is to the dependent's right. A simple one-hidden-layer network that takes as input the GloVe embeddings for the dependent and candidate head as well as distance features between the two words. Our attention-and-words probe, but with attention maps from a BERT network with pre-trained word/positional embeddings but randomly initialized other weights. This kind of baseline is surprisingly strong at other probing tasks . itemize", "Results are shown in Table. We find the Attn + GloVe probing classifier substantially outperforms our baselines and achieves a decent UAS of 77, suggesting BERT's attention maps have a fairly thorough representation of English syntax.", "As a rough comparison, we also report results from the structural probe from , which builds a probing classifier on top of BERT's vector representations rather than attention. The scores are not directly comparable because the structural probe only uses a single layer of BERT, produces undirected rather than directed parse trees, and is trained to produce the syntactic distance between words rather than directly predicting the tree structure. Nevertheless, the similarity in score to our Attn + Glove probing classifier suggests there is not much more syntactic information in BERT's vector representations compared to its attention maps.", "Overall, our results from probing both individual and combinations of attention heads suggest that BERT learns some aspects syntax purely as a by-product of self-supervised training. Other work has drawn a similar conclusions from examining BERT's predictions on agreement tasks or internal vector representations . Traditionally, syntax-aware models have been developed through architecture design (e.g., recursive neural networks) or from direct supervision from human-curated treebanks. Our findings are part of a growing body of work indicating that indirect supervision from rich pre-training tasks like language modeling can also produce models sensitive to language's hierarchical structure.", "Are attention heads in the same layer similar to each other or different? Can attention heads be clearly grouped by behavior? We investigate these questions by computing the distances between all pairs of attention heads. Formally, we measure the distance between two heads head_i and head_j as:", "sum_{ text{token in data} JS( head_i(token), head_j(token)) } Where JS is the Jensen-Shannon Divergence between attention distributions. Using these distances, we visualize the attention heads by applying multidimensional scaling to embed each head in two dimensions such that the Euclidean distance between embeddings reflects the Jensen-Shannon distance between the corresponding heads as closely as possible.", "Results are shown in Figure. We find that there are several clear clusters of heads that behave similarly, often corresponding to behaviors we have already discussed in this paper. Heads within the same layer are often fairly close to each other, meaning that heads within the layer have similar attention distributions. This finding is a bit surprising given that show that encouraging attention heads to have different behaviors can improve Transformer performance at machine translation. One possibility for the apparent redundancy in BERT's attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training.", "There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pre-training. One line of research examines the outputs of language models on carefully chosen input sentences . For example, the model's performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model's syntactic ability, although it does not reveal how that ability is captured by the network.", "Another line of work investigates the internal vector representations of the model , often using probing classifiers. Probing classifiers are simple neural networks that take the vector representations of a pre-trained model as input and are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the input representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated models capturing aspects of syntax or coreference without explicitly being trained for the tasks.", "With regards to analyzing attention, builds a visualization tool for the BERT's attention and reports observations about the attention behavior, but does not perform quantitative analysis. analyze the attention of memory networks to understand model performance on a question answering dataset. %; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. propose heuristic ways of converting attention scores to syntactic trees, but do not quantitatively evaluate their approach. For coreference, show that the attention of a context-aware neural machine translation system captures anaphora, similar to our finding for BERT.", "Concurrently with our work identify syntactic, positional, and rare-word-sensitive attention heads in machine translation models. They also demonstrate that many attention heads can be pruned away without substantially hurting model performance. Interestingly, the important attention heads that remain after pruning tend to be ones with identified behaviors. similarly show that many of BERT's attention heads can be pruned. Although our analysis in this paper only found interpretable behaviors in a subset of BERT's attention heads, these recent works suggest that there might not be much to explain for some attention heads because they have little effect on model perfomance.", "argue that attention often does not \"explain\" model predictions. They show that attention weights frequently do not correlate with other measures of feature importance. Furthermore, attention weights can often be substantially changed without altering model predictions. However, our motivation for looking at attention is different: rather than explaining model predictions, we are seeking to understand information learned by the models. For example, if a particular attention head learns a syntactic relation, we consider that an important finding from an analysis perspective even if that head is not always used when making predictions for some downstream task.", "We have proposed a series of analysis methods for understanding the attention mechanisms of models and applied them to BERT. While most recent work on model analysis for NLP has focused on probing vector representations or model outputs, we have shown that a substantial amount of linguistic knowledge can be found not only in the hidden states, but also in the attention maps. We think probing attention maps complements these other model analysis techniques, and should be part of the toolkit used by researchers to understand what neural networks learn about language.", "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "Self-training methods such as ELMo, GPT, BERT , XLM, and XLNet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. % Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.", "We present a replication study of BERT pretraining, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. % We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-News) of comparable size to other privately used datasets, to better control for training set size effects.", "When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by . Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT's masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling.", "In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC-News, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch.", "In this section, we give a brief overview of the BERT pretraining approach and some of the training choices that we will examine experimentally in the following section.", "BERT takes as input a concatenation of two segments (sequences of tokens), x_1 , ldots , x_N and y_1 , ldots , y_M. Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: is a parameter that controls the maximum sequence length during training.", "The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data.", "BERT uses the now ubiquitous transformer architecture , which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H.", "During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction.", "Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK], 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section ).", "Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text. Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference , which require reasoning about the relationships between pairs of sentences.", "BERT is optimized with Adam using the following parameters: beta_1 = 0.9, beta_2= 0.999, epsilon = 1e-6 and L_2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function. Models are pretrained for S=1,000,000 updates, with minibatches containing B=256 sequences of maximum length T=512 tokens.", "BERT is trained on a combination of BookCorpus plus English Wikipedia, which totals 16GB of uncompressed text.", "We reimplement BERT in fairseq. We primarily follow the original BERT optimization hyperparameters, given in Section, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting beta_2 = 0.98 to improve stability when training with large batch sizes.", "We pretrain with sequences of at most T=512 tokens. Unlike , we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences.", "We train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 times 32GB Nvidia V100 GPUs interconnected by Infiniband.", "BERT-style pretraining crucially relies on large quantities of text. demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT. Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison.", "We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora: itemize[leftmargin=*]", "CC-News, which we collected from the English portion of the CommonCrawl News dataset. The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).", "OpenWebText, an open-source recreation of the WebText corpus described in. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).", "Stories, a dataset introduced in containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB). itemize", "Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks.", "The General Language Understanding Evaluation (GLUE) benchmark is a collection of 9 datasets for evaluating natural language understanding systems. Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data.", "For the replication study in Section, we report results on the development sets after finetuning the pretrained models on the corresponding single-task training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper.", "In Section we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section.", "SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0. In V1.1 the context always contains an answer, whereas in V2.0 some questions are not answered in the provided context, making the task more challenging.", "For SQuAD V1.1 we adopt the same span prediction method as BERT. For SQuAD V2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms. During evaluation, we only predict span indices on pairs that are classified as answerable.", "RACE The ReAding Comprehension from Examinations (RACE) task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle and high school students. In RACE, each passage is associated with multiple questions. For every question, the task is to select one correct answer from four options. RACE has significantly longer context than other popular reading comprehension datasets and the proportion of questions that requires reasoning is very large.", "This section explores and quantifies which choices are important for successfully pretraining BERT models. We keep the model architecture fixed. Specifically, we begin by training BERT models with the same configuration as BERT_{base} (L=12, H=768, A=12, 110M params).", "As discussed in Section , BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training.", "We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets.", "Table compares the published BERT_{base} results from to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking.", "Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments.", "In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p=0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss.", "The NSP loss was hypothesized to be an important factor in training the original BERT model. observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1. However, some recent work has questioned the necessity of the NSP loss.", "To better understand this discrepancy, we compare several alternative training formats: itemize[leftmargin=*]", "segment-pair+nsp: This follows the original input format used in BERT, with the NSP loss. Each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens.", "sentence-pair+nsp: Each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. Since these inputs are significantly shorter than 512 tokens, we increase the batch size so that the total number of tokens remains similar to segment-pair+nsp. We retain the NSP loss.", "full-sentences: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss.", "doc-sentences: Inputs are constructed similarly to full-sentences, except that they may not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as full-sentences. We remove the NSP loss. itemize", "Table shows results for the four different settings. We first compare the original segment-pair input format from to the sentence-pair format; both formats retain the NSP loss, but the latter uses single sentences. We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.", "We next compare training without the NSP loss and training with blocks of text from a single document (doc-sentences). We find that this setting outperforms the originally published BERT_{base} results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to . It is possible that the original BERT implementation may only have removed the loss term while still retaining the segment-pair input format.", "Finally we find that restricting sequences to come from a single document (doc-sentences) performs slightly better than packing sequences from multiple documents (full-sentences). However, because the doc-sentences format results in variable batch sizes, we use full-sentences in the remainder of our experiments for easier comparison with related work.", "Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately. Recent work has shown that BERT is also amenable to large batch training.", "originally trained BERT_{base} for 1M steps with a batch size of 256 sequences. This is equivalent in computational cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K.", "In Table we compare perplexity and end-task performance of BERT_{base} as we increase the batch size, controlling for the number of passes through the training data. We observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training, and in later experiments we train with batches of 8K sequences.", "Notably train BERT with even larger batche sizes, up to 32K sequences. We leave further exploration of the limits of large batch training to future work.", "Byte-Pair Encoding (BPE) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus.", "BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any \"unknown\" tokens.", "The original BERT implementation uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following , we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERT_{base} and BERT_{large}, respectively.", "Early experiments revealed only slight differences between these encodings, with the BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work.", "In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section), full-sentences without NSP loss (Section), large mini-batches (Section) and a larger byte-level BPE (Section).", "Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture is pretrained using nearly 10 times more data than the original BERT. It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT.", "To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERT_{large} architecture (L=24, H=1024, A=16, 355M parameters). We pretrain for 100K steps over a comparable BookCorpus plus Wikipedia dataset as was used in . We pretrain our model using 1024 V100 GPUs for approximately one day.", "We present our results in Table. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERT_{large} results, reaffirming the importance of the design choices we explored in Section.", "Next, we combine this data with the three additional datasets described in Section. We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.", "Finally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNet_{large} across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training.", "In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section.", "For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes in {16, 32 } and learning rates in {1e-5, 2e-5, 3e-5 }, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task's evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling.", "In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard. While many submissions to the GLUE leaderboard depend on multi-task finetuning, our submission depends only on single-task finetuning. For RTE, STS and MRPC we found it helpful to finetune starting from the MNLI single-task model, rather than the baseline pretrained RoBERTa. We explore a slightly wider hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models per task.", "Two of the GLUE tasks require task-specific finetuning approaches to achieve competitive leaderboard results.", "QNLI: Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classified as positive. This formulation significantly simplifies the task, but is not directly comparable to BERT. Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification approach.", "WNLI: We found the provided NLI-format data to be challenging to work with. Instead we use the reformatted WNLI data from SuperGLUE, which indicates the span of the query pronoun and referent. We finetune RoBERTa using the margin ranking loss from . For a given input sentence, we use spaCy to extract additional candidate noun phrases from the sentence and finetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases. One unfortunate consequence of this formulation is that we can only make use of the positive training examples, which excludes over half of the provided training examples.", "We present our results in Table. In the first setting (single-task, dev), RoBERTa achieves state-of-the-art results on all 9 of the GLUE task development sets. Crucially, RoBERTa uses the same masked language modeling pretraining objective and architecture as BERT_{large}, yet consistently outperforms both BERT_{large} and XLNet_{large}. This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work.", "In the second setting (ensembles, test), we submit RoBERTa to the GLUE leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. This is especially exciting because RoBERTa does not depend on multi-task finetuning, unlike most of the other top submissions. We expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures.", "We adopt a much simpler approach for SQuAD compared to past work. In particular, while both BERT and XLNet augment their training data with additional QA datasets, we only finetune RoBERTa{ using the provided SQuAD training data}. also employed a custom layer-wise learning rate schedule to finetune XLNet, while we use the same learning rate for all layers.", "For SQuAD v1.1 we follow the same finetuning procedure as . For SQuAD v2.0, we additionally classify whether a given question is answerable; we train this classifier jointly with the span predictor by summing the classification and span loss terms.", "We present our results in Table. On the SQuAD v1.1 development set, RoBERTa matches the state-of-the-art set by XLNet. On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1).", "We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT or XLNet, both of which rely on additional external training data. In contrast, our submission does not use any additional data.", "Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.", "In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct.", "We modify RoBERTa for this task by concatenating each candidate answer with the corresponding question and passage. We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.", "Results on the RACE test sets are presented in Table. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings.", "Pretraining methods have been designed with different training objectives, including language modeling, machine translation, and masked language modeling. Many recent papers have used a basic recipe of finetuning models for each end task, and pretraining with some variant of a masked language model objective. However, newer methods have improved performance by multi-task fine tuning, incorporating entity embeddings, span prediction, and multiple variants of autoregressive pretraining. Performance is also typically improved by training bigger models on more data. Our goal was to replicate, simplify, and better tune the training of BERT, as a reference point for better understanding the relative performance of all of these methods.", "We carefully evaluate a number of design decisions when pretraining BERT models. We find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task finetuning for GLUE or additional data for SQuAD. These results illustrate the importance of these previously overlooked design decisions and suggest that BERT's pretraining objective remains competitive with recently proposed alternatives.", "We additionally use a novel dataset, CC-News, and release our models and code for pretraining and finetuning at: https://github.com/pytorch/fairseq.", "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at https://github.com/jackroos/VL-BERT.", "Pre-training of generic feature representations applicable to a variety of tasks in a domain is a hallmark of the success of deep networks. Firstly in computer vision, backbone networks designed for and pre-trained on ImageNet classification are found to be effective for improving numerous image recognition tasks. Recently in natural language processing (NLP), Transformer networks pre-trained with \"masked language model\" (MLM) objective on large language corpus excel at a variety of NLP tasks.", "Meanwhile, for tasks at the intersection of vision and language, such as image captioning, visual question answering (VQA), visual commonsense reasoning (VCR), there lacks such pre-trained generic feature representations. The previous practice is to combine base networks pre-trained for image recognition and NLP respectively in a task-specific way. The task-specific model is directly finetuned for the specific target task, without any generic visual-linguistic pre-training. The task-specific model may well suffer from overfitting when the data for the target task is scarce. Also, due to the task-specific model design, it is difficult to benefit from pre-training, where the pre-training task may well be different from the target. There lacks a common ground for studying the feature design and pre-training of visual-linguistic tasks in general.", "In the various network architectures designed for different visual-linguistic tasks, a key goal is to effectively aggregate the multi-modal information in both the visual and linguistic domains. For example, to pick the right answer in the VQA task, the network should empower integrating linguistic information from the question and the answers, and aggregating visual information from the input image, together with aligning the linguistic meanings with the visual clues. Thus, we seek to derive generic representations that can effectively aggregate and align visual and linguistic information.", "In the meantime, we see the successful application of Transformer attention in NLP, together with its MLM-based pre-training technique in BERT. The attention module is powerful and flexible in aggregating and aligning word embedded features in sentences, while the pre-training in BERT further enhances the capability.", "Inspired by that, we developed VL-BERT, a pre-trainable generic representation for visual-linguistic tasks, as shown in Figure. The backbone of VL-BERT is of (multi-modal) Transformer attention module taking both visual and linguistic embedded features as input. In it, each element is either of a word from the input sentence, or a region-of-interest (RoI) from the input image, together with certain special elements to disambiguate different input formats. Each element can adaptively aggregate information from all the other elements according to the compatibility defined on their contents, positions, categories, and etc. The content features of a word / an RoI are domain specific (WordPiece embeddings as word features, Fast R-CNN features for RoIs). By stacking multiple layers of multi-modal Transformer attention modules, the derived representation is of rich capability in aggregating and aligning visual-linguistic clues. And task-specific branches can be added above for specific visual-linguistic tasks.", "To better exploit the generic representation, we pre-train VL-BERT at both large visual-linguistic corpus and text-only datasets. The pre-training loss on the visual-linguistic corpus is incurred via predicting randomly masked words or RoIs. Such pre-training sharpens the capability of VL-BERT in aggregating and aligning visual-linguistic clues. While the loss on the text-only corpus is of the standard MLM loss in BERT, improving the generalization on long and complex sentences.", "Comprehensive empirical evidence demonstrates that the proposed VL-BERT achieves state-of-the-art performance on various downstream visual-linguistic tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. In particular, we achieved the first place of single model on the leaderboard of visual commonsense reasoning.", "Pre-training for Computer Vision Prior to the era of deep networks, it is far from mature to share features among different tasks and to improve the features via pre-training. The models for various computer vision tasks are of too diverse design choices to derive a generic representation. With the success of AlexNet in ImageNet classification, we see the renaissance of convolutional neural networks (CNNs) in the vision community. Soon after that, researchers found that ImageNet pre-trained CNNs can serve well as generic feature representation for various downstream tasks , such as object detection, semantic segmentation, instance segmentation. The improvement in backbone networks for ImageNet classification further improves the downstream tasks. Recently there are research works on directly training CNNs from scratch on massive-scale target datasets, without ImageNet pre-training. They achieved performance on par with those with ImageNet pre-training. While they also note that pre-training on a proper massive dataset is vital for improving performance on target tasks with scarce data.", "Pre-training for Natural Language Processing (NLP) It is interesting to note that the development of pre-training techniques in NLP lags quite behind computer vision. There are previous research works on improving word embedding, which is a low-level linguistic feature representation. On top of that, numerous diverse architectures are designed for various NLP tasks. In the milestone work of Transformers, the Transformer attention module is proposed as a generic building block for various NLP tasks. After that, a serious of approaches are proposed for pre-training the generic representation, mainly based on Transformers, such as GPT, BERT, GPT-2, XLNet, XLM, and RoBERTa. Among them, BERT is perhaps the most popular one due to its simplicity and superior performance.", "Pre-training for Visual-Linguistic Tasks. The development course of models for visual-linguistic tasks is also quite similar to those in the computer vision and NLP communities. Previously, task-specific models are designed, wherein the features derived from off-the-shelf computer vision and NLP models are combined in an ad-hoc way for specific tasks. Model training is performed on the dataset for the specific task only.", "VideoBERT is the first work seeking to conduct pre-training for visual-linguistic tasks. In it, video clips are processed by off-the-shelf networks for action recognition, and are assigned to different clusters (visual words) based on the derived features. The pre-training loss is incurred via predicting the cluster ids of masked video clips. Due to the abrupt clustering of the video clips, it losses considerable visual content information and hinders updating visual network parameters. In the following work of CBT, such clustering mechanism is removed. Both works are applied on videos, which are of linear structure in the time dimension, same as sentences. It is highly desired to study at the well-established image-based visual-linguistic tasks.", "Concurrent to our work, multiple works released on Arxiv very recently also seek to derive a pre-trainable generic representation for visual-linguistic tasks. Table in Appendix compares among them. We briefly discuss some of these works here.", "In ViLBERT and LXMERT, which are under review or just got accepted, the network architectures are of two single-modal networks applied on input sentences and images respectively, followed by a cross-modal Transformer combining information from the two sources. The attention pattern in the cross-modal Transformer is restricted, where the authors believe to improve the performance. The authors of ViLBERT claim that such two-stream design is superior than a single-stream unified model. Meanwhile, in the proposed VL-BERT, it is of a unified architecture based on Transformers without any restriction on the attention patterns. The visual and linguistic contents are fed as input to VL-BERT, wherein they interact early and freely. We found that our unified model of VL-BERT outperforms such two-stream designs.", "VisualBert, B2T2, and Unicoder-VL, which are of work in progress or under review, are also of unified single-stream architecture. The differences of these works are compared in Table. The concurrent emergency of these research works indicates the importance of deriving a generic pre-trainable representation for visual-linguistic tasks.", "In addition, there are three noticeable differences between VL-BERT and other concurrent works in pre-training. Their effects are validated in Section. (1) We found the task of Sentence-Image Relationship Prediction used in all of the other concurrent works (e.g., ViLBERT and LXMERT) is of no help in pre-training visual-linguistic representations. Thus such a task is not incorporated in VL-BERT. (2) We pre-train VL-BERT on both visual-linguistic and text-only datasets. We found such joint pre-training improves the generalization over long and complex sentences. (3) Improved tuning of the visual representation. In VL-BERT, the parameters of Fast R-CNN, deriving the visual features, are also updated. To avoid visual clue leakage in the pre-training task of Masked RoI Classification with Linguistic Clues, the masking operation is conducted on the input raw pixels, other than the feature maps produced by layers of convolution.", "Let x = {x_1, ..., x_N } be the input elements in BERT, which are of embedded features encoding sentence words. They are processed by a multi-layer bidirectional Transformer, where the embedding features of each element are transformed layer-by-layer in the fashion of aggregating features from the other elements with adaptive attention weights. Let x^l = {x^l_1, ..., x^l_N } be the features of the l-th layer (x^0 is set as the input x). The features of the (l+1)-th layer, x^{l+1}, is computed by", "where m in Eq. indexes over the attention heads, and denotes the attention weights between elements i and j in the m-th head, which is normalized by sum_{j=1}^{N}A^{m}_{i,j} = 1. W^{l+1}_m, Q^{l+1}_m, K^{l+1}_m and V^{l+1}_m are learnable weights for m^th attention head, W_1^{l+1}, W_2^{l+1} and b_1^{l+1}, b_2^{l+1} in Eq. are learnable weights and biases, respectively. Note that, the operations in Eq.~ sim is irrelevant to the order of input sequence, i.e. the final BERT representation of permuted input is same as the final BERT representation of the original input after the same permutation. The position of an element in BERT is encoded in its own embedding features by sequence positional embedding. Thanks to such decoupled representation, the BERT model is flexible enough to be pre-trained and finetuned for a variety of NLP tasks.", "In BERT pre-training, the masked language modeling (MLM) task is introduced. The embedded features of a certain input word would be randomly masked out (the token embedding channels capturing the word content is replaced by a special [MASK] token). The BERT model is trained to predict the masked word from linguistic clues of all the other unmasked elements. As explained in , the overall MLM-based training of BERT is equivalent to optimizing the following joint probability distribution", "where phi_i (x| theta) is the potential function for the i-th input element, with parameters theta, and Z( theta) is the partition function. Each log-potential term log phi_i (x) is defined as", "where f_i(x_{ setminus i} | theta) denotes the final output feature of BERT corresponding to the i-th element for input x_{ setminus i}, where x_{ setminus i} is defined as . The incurred MLM-based loss is as", "where x is a randomly sampled sentence from the training set D, and i is a randomly sampled location for masking words.", "The second pre-training task, Next Sentence Prediction, focuses on modeling the relationship between two sentences. Two sentences are sampled from the input document, and the model should predict whether the second sentence is the direct successor of the first. In BERT, the sampled two sentences are concatenated into one input sequence, with special elements [CLS] and [SEP] inserted prior to the first and the second sentences, respectively. A Sigmoid classifier is appended on the final output feature corresponding to the [CLS] element to make the prediction. Let x be the input sequence, t in {0,1 } indicates the relationship between the two sentences. The loss function is defined as", "where x_0^L is the final output feature of the [CLS] element (at the L-th layer), and g(x_0^L) is the classifier output.", "Figure illustrates the architecture of VL-BERT. Basically, it modifies the original BERT model by adding new elements to accommodate the visual contents, and a new type of visual feature embedding to the input feature embeddings. Similar to BERT, the backbone is of multi-layer bidirectional Transformer encoder, enabling dependency modeling among all the input elements. Different to BERT processing sentence words only, VL-BERT takes both visual and linguistic elements as input, which are of features defined on regions-of-interest (RoIs) in images and sub-words from input sentences, respectively. The RoIs can either be bounding boxes produced by object detectors, or be annotated ones in certain tasks.", "It is worth noting that the input formats vary for different visual-linguistic tasks (e.g., <Caption, Image> for image captioning, and <Question, Answer, Image> for VQA and VCR). But thanks to the unordered representation nature of Transformer attention (e.g., the position of a word in sentence is encoded by the positional embedding only, other than the order in the input sequence), a generic representation can be derived as long as the input elements and embedding features are properly designed. Three types of input elements are involved, namely, visual, linguistic, and special elements for disambiguating different input formats. The input sequence always starts with a special classification element ([CLS]), then goes on with linguistic elements, then follows up with visual elements, and ends with a special ending element ([END]). A special separation element ([SEP]) is inserted in between different sentences in the linguistic elements, and between the linguistic and visual elements. For each input element, its embedding feature is the summation of four types of embedding, namely, token embedding, visual feature embedding, segment embedding, and sequence position embedding. Among them, the visual feature embedding is newly introduced for capturing visual clues, while the other three embeddings follow the design in the original BERT paper.", "Token Embedding Following the practice in BERT, the linguistic words are embedded with WordPiece embeddings with a 30,000 vocabulary. A special token is assigned to each special element. For the visual elements, a special [IMG] token is assigned for each one of them.", "Visual Feature Embedding We firstly describe visual appearance feature and visual geometry embedding separately, and then how to combine them to form the visual feature embedding.", "For the visual element corresponding to an RoI, the visual appearance feature is extracted by applying a Fast R-CNN detector (i.e., the detection branch in Faster R-CNN), where the feature vector prior to the output layer of each RoI is utilized as the visual feature embedding (of 2048-d in paper). For the non-visual elements, the corresponding visual appearance features are of features extracted on the whole input image. They are obtained by applying Faster R-CNN on an RoI covering the whole input image.", "The visual geometry embedding is designed to inform VL-BERT the geometry location of each input visual element in image. Each RoI is characterized by a 4-d vector, as denote the coordinate of the top-left and bottom-right corner respectively, and W, H are of the width and height of the input image. Following the practice in Relation Networks, the 4-d vector is embedded into a high-dimensional representation (of 2048-d in paper) by computing sine and cosine functions of different wavelengths.", "The visual feature embedding is attached to each of the input elements, which is the output of a fully connected layer taking the concatenation of visual appearance feature and visual geometry embedding as input.", "Segment Embedding Three types of segment, A, B, C, are defined to separate input elements from different sources, namely, A and B for the words from the first and second input sentence respectively, and C for the RoIs from the input image. For example, for input format of <Question, Answer, Image>, A denotes Question, B denotes Answer, and C denotes Image. For input format of <Caption, Image>, A denotes Caption, and C denotes Image. A learned segment embedding is added to every input element for indicating which segment it belongs to.", "Sequence Position Embedding~ A learnable sequence position embedding is added to every input element indicating its order in the input sequence, same as BERT. Because there is no natural order among input visual elements, any permutation of them in the input sequence should achieve the same result. Thus the sequence position embedding for all visual elements are the same.", "The generic feature representation of VL-BERT enables us to pre-train it on massive-scale datasets, with properly designed pre-training tasks. We pre-train VL-BERT on both visual-linguistic and text-only datasets. Here we utilize the Conceptual Captions dataset as the visual-linguistic corpus. It contains around 3.3 million images annotated with captions, which are harvested from web data and processed through an automatic pipeline. The issue with the Conceptual Captions dataset is that the captions are mainly simple clauses, which are too short and simple for many down-stream tasks. To avoid overfitting on such short and simple text scenario, we also pre-train VL-BERT on text-only corpus with long and complex sentences. We utilize the BooksCorpus and the English Wikipedia datasets, which are also utilized in pre-training BERT.", "In SGD training, in each mini-batch, samples are randomly drawn from both Conceptual Captions and BooksCorpus & English Wikipedia (at a ratio of 1:1). For a sample drawn from Conceptual Captions, the input format to VL-BERT is of <Caption, Image>, where the RoIs in the image are localized and categorized by a pre-trained Faster R-CNN object detector. Two pre-training tasks are exploited to incur loss, which are as follows.", "Task #1 colon Masked Language Modeling with Visual Clues This task is very similar to the Masked Language Modeling (MLM) task utilized in BERT. The key difference is that visual clues are incorporated in VL-BERT for capturing the dependencies among visual and linguistic contents. During pre-training, each word in the input sentence(s) is randomly masked (at a probability of 15%). For the masked word, its token is replaced with a special token of [MASK]. The model is trained to predict the masked words, based on the unmasked words and the visual features. The task drives the network to not only model the dependencies in sentence words, but also to align the visual and linguistic contents. For example, in Figure \"kitten drinking from [MASK]\", without the input image, the masked word could be any containers, such as \"bowl\", \"spoon\" and \"bottle\". The representation should capture the correspondence of the word \"bottle\" and the corresponding RoIs in the image to make the right guess. During pre-training, the final output feature corresponding to the masked word is fed into a classifier over the whole vocabulary, driven by Softmax cross-entropy loss.", "Task #2 colon Masked RoI Classification with Linguistic Clues This is a dual task of Task #1. Each RoI in image is randomly masked out (with 15% probability), and the pre-training task is to predict the category label of the masked RoI from the other clues. To avoid any visual clue leakage from the visual feature embedding of other elements, the pixels laid in the masked RoI are set as zeros before applying Fast R-CNN. During pre-training, the final output feature corresponding to the masked RoI is fed into a classifier with Softmax cross-entropy loss for object category classification. The category label predicted by pre-trained Faster R-CNN is set as the ground-truth. An example is shown in Figure. The RoI corresponding to cat in image is masked out, and the corresponding category cannot be predicted from any visual clues. But with the input caption of \"kitten drinking from bottle\", the model can infer the category by exploiting the linguistic clues.", "For a sample drawn from the BooksCorpus & English Wikipedia datasets, the input format to VL-BERT degenerates to be <Text, varnothing>, where no visual information is involved. The \"visual feature embedding\" term in Figure is a learnable embedding shared for all words. The training loss is from the standard task of Masked Language Modeling (MLM) as in BERT.", "In summary, the pre-training on visual-linguistic corpus improves the detailed alignment between visual and linguistic contents. Such detailed alignment is vital for many downstream tasks (for example, in Visual Grounding, the model locates the most relevant object or region in an image based on a natural language query). While the pre-training on text-only corpus facilitates downstream tasks involving understanding of long and complex sentences.", "VL-BERT is designed to be a generic feature representation for various visual-linguistic tasks. It is relatively simple to finetune VL-BERT for various downstream tasks. We simply need to feed VL-BERT with properly formatted input and output, and finetune all the network parameters end-to-end. For the input, the typical formats of <Caption, Image> and <Question, Answer, Image> cover the majority visual-linguistic tasks. VL-BERT also supports more sentences and more images as long as appropriate segment embeddings are introduced to identify different input sources. At the output, typically, the final output feature of the [CLS] element is used for sentence-image-relation level prediction. The final output features of words or RoIs are for word-level or RoI-level prediction. In addition to the input and output format, task-specific loss functions and training strategies also need to be tuned. See Section for the detailed design choices and settings.", "As described in Section, we pre-train VL-BERT jointly on Conceptual Captions as visual-linguistic corpus, and BooksCorpus & English Wikipedia as text-only corpus. As VL-BERT is developed via adding new inputs capturing visual information to the original BERT model, we initialize the parameters to be the same as the original BERT described in . VL-BERT_BASE and VL-BERT_LARGE denote models developed from the original BERT_BASE and BERT_LARGE models, respectively. The newly added parameters in VL-BERT are randomly initialized from a Gaussian distribution with mean of 0 and standard deviation of 0.02. Visual content embedding is produced by Faster R-CNN + ResNet-101, initialized from parameters pre-trained on Visual Genome for object detection (see BUTD).", "Prior to pre-training on Conceptual Captions, the pre-trained Faster R-CNN is applied to extract RoIs. Specifically, at most 100 RoIs with detection scores higher than 0.5 are selected for each image. At minimum, 10 RoIs are selected from one image, regardless of the detection score threshold. The detailed parameter settings are in Appendix.", "The pre-trained VL-BERT model can be fine-tuned for various downstream visual-linguistic tasks, with simple modifications on the input format, output prediction, loss function and training strategy.", "Visual Commonsense Reasoning (VCR) focuses on higher-order cognitive and commonsense understanding of the given image. In the dataset of , given an image and a list of categorized RoIs, a question at cognition level is raised. The model should pick the right answer to the question and provide the rationale explanation. For each question, there are 4 candidate answers and 4 candidate rationales. This holistic task (Q rightarrow AR) is decomposed into two sub-tasks wherein researchers can train specific individual models: question answering (Q rightarrow A) and answer justification (QA rightarrow R). The released VCR dataset consists of 265k pairs of questions, answers, and rationales, over 100k unique movie scenes (100k images). They are split into training, validation, and test sets consisting of 213k questions and 80k images, 27k questions and 10k images, and 25k questions and 10k images, respectively.", "Our experimental protocol for VCR follows that in R2C. The model is trained on the train split, and is evaluated at the val and test sets. In the original work R2C, task-specific \"Grounding\", \"Contextualization\" and \"Reasoning\" modules are designed. Here we simply adopt the generic representation of VL-BERT for the task. Figure (a) illustrates the input format, <Question, Answer, Image>. For the sub-task of Q rightarrow A, 'Q' and 'A' are filled to the Question section and Answer section respectively. For the sub-task of QA rightarrow R , the concatenation of 'Q' and 'A' is filled to the Question section, and 'R' is filled to the Answer section. The input RoIs to VL-BERT are the ground-truth annotations in the dataset. The final output feature of [CLS] element is fed to a Softmax classifier for predicting whether the given Answer is the correct choice. During fine-tuning, we adopt two losses, the classification over the correctness of the answers and the RoI classification with linguistic clues. The detailed parameter settings are in Appendix.", "Table presents the experiment results. Pre-training VL-BERT improves the performance by 1.0% in the final Q rightarrow AR task, which validates the effectiveness of pre-training. Compared with R2C, we do not use ad-hoc task-specific modules. Instead, we simply adopt the generic representation of VL-BERT and jointly train the whole model end-to-end. Despite the same input, output and experimental protocol as R2C, VL-BERT outperforms R2C by large margins, indicating the power of our simple cross-modal architecture. Compared with other concurrent works, i.e., ViLBERT, VisualBERT and B2T2, our VL-BERT achieves the state-of-the-art performance.", "In the VQA task, given a natural image, a question at the perceptual level is asked, and the algorithm should generate / choose the correct answer. Here we conduct experiments on the widely-used VQA v2.0 dataset, which is built based on the COCO images. The VQA v2.0 dataset is split into train (83k images and 444k questions), validation (41k images and 214k questions), and test (81k images and 448k questions) sets. Following the experimental protocol in BUTD, for each question, the algorithm should pick the corresponding answer from a shared set consisting of 3,129 answers.", "Figure (b) illustrates the input format for the VQA task, which is of <Question, Answer, Image>. As the possible answers are from a shared pool independent to the question, we only fill a [MASK] element to the Answer section. As in BUTD, the input RoIs in VL-BERT are generated by a Faster R-CNN detector pre-trained on Visual Genome. The answer prediction is made from a multi-class classifier based upon the output feature of the [MASK] element. During fine-tuning, the network training is driven by the multi-class cross-entropy loss over the possible answers. The detailed parameter settings are in Appendix.", "Table presents our experimental results. Pre-training VL-BERT improves the performance by 1.6%, which validates the importance of pre-training. VL-BERT shares the same input (i.e., question, image, and RoIs), output and experimental protocol with BUTD, a prevalent model specifically designed for the task. Still, VL-BERT surpasses BUTD by over 5% in accuracy. Except for LXMERT, our VL-BERT achieves better performance than the other concurrent works. This is because LXMERT is pre-trained on massive visual question answering data (aggregating almost all the VQA datasets based on COCO and Visual Genome). While our model is only pre-trained on captioning and text-only dataset, where there is still gap with the VQA task.", "A referring expression is a natural language phrase that refers to an object in an image. The referring expression comprehension task is to localize the object in an image with the given referring expression. We adopt the RefCOCO+ dataset for evaluation, consisting of 141k expressions for 50k referred objects in 20k images in the COCO dataset. The referring expressions in RefCOCO+ are forbidden from using absolute location words, e.g. left dog. Therefore the referring expressions focus on purely appearance-based descriptions. RefCOCO+ are split into four sets, training set (train), validation set (val), and two testing sets (testA and testB). Images containing multiple people are in testA set, while images containing multiple objects of other categories are in testB set. There is no overlap between the training, validation and testing images.", "Figure (c) illustrates the input format for referring expression comprehension , where the input format is of <Query, Image>. Model training and evaluation are conducted either on the ground-truth RoIs or on the detected boxes in MAttNet. And the results are reported either in the track of ground-truth regions or that of detected regions, respectively. During training, we compute the classification scores for all the input RoIs. For each RoI, a binary classification loss is applied. During inference, we directly choose the RoI with the highest classification score as the referred object of the input referring expression. The detailed parameter settings are in Appendix.", "Table presents our experimental results. Pre-trained VL-BERT significantly improves the performance. Compared with MAttNet, VL-BERT is much simpler without task-specific architecture designs, yet much better. VL-BERT achieves comparable performance with the concurrent work of ViLBERT.", "Table ablates key design choices in pre-training VL-BERT. For experimental efficiency, the finetuning epoches of VL-BERT are of 0.5 times of those in Section, with only VL-BERT_BASE model.", "Overall, the pre-training of VL-BERT improves the performance over all the three down-stream tasks (by comparing setting \"w/o pre-training\" and VL-BERT_BASE). The improvement amplitude varies for different tasks. By comparing setting (a) to that of \"w/o pre-training\", we see the benefits of Task #1, Masked Language Modeling with Visual Clues. By further incorporating Task #2, Masked RoI Classification with Linguistic Clues, the accuracy further improves on RefCOCO+, but gets stuck at VCR and VQA. This might be because only RefCOCO+ utilizes the final output feature corresponding to [IMG] tokens for prediction. Thus the pre-training of such features is beneficial. Setting (c) incorporates the task of Sentence-Image Relationship Prediction as in ViLBERT and LXMERT. It would hurt accuracy on all the three down-stream tasks. We guess the reason is because the task of Sentence-Image Relationship Prediction would introduce unmatched image and caption pairs as negative examples. Such unmatched samples would hamper the training of other tasks. Setting (d) adds text-only corpus during pre-training. Compared with setting (b), it improves the performance over all three down-stream tasks, and is most significant on VCR. This is because the task of VCR involves more complex and longer sentences than those in VQA and RefCOCO+. By further finetuning the network parameters of Fast R-CNN, which generates the visual features, we get the final setting of VL-BERT_BASE. Such end-to-end training of the entire network is helpful for all the downstream tasks.", "In this paper, we developed VL-BERT, a new pre-trainable generic representation for visual-linguistic tasks. Instead of using ad-hoc task-specific modules, VL-BERT adopts the simple yet powerful Transformer model as the backbone. It is pre-trained on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues, and thus benefit the downstream tasks. In the future, we would like to seek better pre-training tasks, which could beneficial more downstream tasks (e.g., Image Caption Generation).", "subsubsection*{Acknowledgments} The work is partially supported by the National Natural Science Foundation of China under grand No.U19B2044 and No.61836011.", "BERT and RoBERTa has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~ 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.", "In this publication, we present Sentence-BERT (SBERT), a modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. This enables BERT to be used for certain new tasks, which up-to-now were not applicable for BERT. These tasks include large-scale semantic similarity comparison, clustering, and information retrieval via semantic search.", "BERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of n=10 ,000 sentences the pair with the highest similarity requires with BERT n cdot(n-1)/2=49 ,995 ,000 inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.", "A common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed-size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings .", "To alleviate this issue, we developed SBERT. The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosine-similarity or Manhatten / Euclidean distance, semantically similar sentences can be found. These similarity measures can be performed extremely efficient on modern hardware, allowing SBERT to be used for semantic similarity search as well as for clustering. The complexity for finding the most similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~ 5 seconds with SBERT) and computing cosine-similarity (~ 0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds .", "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent and Universal Sentence Encoder . On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval , an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.", "SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article .", "The paper is structured in the following way: Section presents SBERT, section evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus . Section evaluates SBERT on SentEval. In section , we perform an ablation study to test some design aspect of SBERT. In section , we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods.", "BERT is a pre-trained transformer network , which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a new state-of-the-art performance on the Semantic Textual Semilarity (STS) benchmark . RoBERTa showed, that the performance of BERT can further improved by small adaptations to the pre-training process. We also tested XLNet , but it led in general to worse results than BERT.", "A large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: ). These two options are also provided by the popular bert-as-a-service-repository. Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings.", "Sentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought trains an encoder-decoder architecture to predict the surrounding sentences. InferSent uses labeled data of the Stanford Natural Language Inference dataset and the Multi-Genre NLI dataset to train a siamese BiLSTM network with max-pooling over the output. Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder trains a transformer network and augments unsupervised learning with training on SNLI. showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work found that the SNLI datasets are suitable for training sentence embeddings. presented a method to train on conversations from Reddit using siamese DAN and siamese transformer networks, which yielded good results on the STS benchmark dataset.", "addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between m context vectors and pre-computed candidate embeddings using attention. This idea works for finding the highest scoring sentence in a larger collection. However, poly-encoders have the drawback that the score function is not symmetric and the computational overhead is too large for use-cases like clustering, which would require O(n^2) score computations.", "Previous neural sentence embedding methods started the training from a random initialization. In this publication, we use the pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings. This reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods.", "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.", "In order to fine-tune BERT / RoBERTa, we create siamese and triplet networks to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.", "The network structure depends on the available training data. We experiment with the following structures and objective functions.", "Classification Objective Function. We concatenate the sentence embeddings u and v with the element-wise difference |u-v| and multiply it with the trainable weight o = softmax(W_t (u, v, |u-v|)) is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure .", "Regression Objective Function. The cosine-similarity between the two sentence embeddings u and v is computed (Figure ). We use mean-squared-error loss as the objective function.", "Triplet Objective Function. Given an anchor sentence a, a positive sentence p, and a negative sentence n, triplet loss tunes the network such that the distance between a and p is smaller than the distance between a and n. Mathematically, we minimize the following loss function: max(||s_a-s_p|| - ||s_a-s_n|| + epsilon, 0). As metric we use Euclidean distance and we set epsilon=1 in our experiments.", "We train SBERT on the combination of the SNLI and the Multi-Genre NLI dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels contradiction, eintailment, and neutral. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate 2e{-5}, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN.", "We evaluate the performance of SBERT for common Semantic Textual Similarity (STS) tasks. State-of-the-art methods often learn a (complex) regression function that maps sentence embeddings to a similarity score. However, these regression functions work pair-wise and due to the combinatorial explosion those are often not scalable if the collection of sentences reaches a certain size. Instead, we always use cosine-similarity to compare the similarity between two sentence embeddings. We ran our experiments also with negative Manhatten and negative Euclidean distances as similarity measures, but the results for all approaches remained roughly the same.", "We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 , the STS benchmark , and the SICK-Relatedness dataset . These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. The setup for the other sentence embedding methods is equivalent, the similarity is computed by cosine-similarity. The results are depicted in Table .", "The results shows that directly using the output of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLS-token output only achieves an average correlation of 29.19. Both are worse than computing average GloVe embeddings.", "Using the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.", "While RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings.", "The STS benchmark (STSb) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories captions, news, and forums. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regression method for the output.", "We use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances .", "The results are depicted in Table . We experimented with two setups: Only training on STSb, and first training on NLI, then training on STSb. We observe that the later strategy leads to a slight improvement of 1-2 points. This two-step approach had an especially large impact for the BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a significant difference between BERT and RoBERTa.", "We evaluate SBERT on the Argument Facet Similarity (AFS) corpus by . The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: gun control, gay marriage, and death penalty. The data was annotated on a scale from 0 (\"different topic\") to 5 (\"completely equivalent\"). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset .", "We evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A draw-back of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results.", "SBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation r to make the results comparable to Misra et al. However, we showed that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table .", "Unsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores. Training SBERT in the 10-fold cross-validation setup gives a performance that is nearly on-par with BERT.", "However, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7 points Spearman correlation. To be considered similar, arguments should address the same claims and provide the same reasoning. BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT.", "use Wikipedia to create a thematically fine-grained train, dev and test set for sentence embeddings methods. Wikipedia articles are separated into distinct sections focusing on certain aspects. Dor et al. assume that sentences in the same section are thematically closer than sentences in different sections. They use this to create a large dataset of weakly labeled sentence triplets: The anchor and the positive example come from the same section, while the negative example comes from a different section of the same article. For example, from the Alice Arnold article: Anchor: Arnold joined the BBC Radio Drama Company in 1988., positive: Arnold gained media attention in May 2012., negative: Balding and Arnold are keen amateur golfers.", "We use the dataset from Dor et al. We use the Triplet Objective, train SBERT for one epoch on the about 1.8 Million training triplets and evaluate it on the 222,957 test triplets. Test triplets are from a distinct set of Wikipedia articles. As evaluation metric, we use accuracy: Is the positive example closer to the anchor than the negative example?", "Results are presented in Table . Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al.", "SentEval is a popular toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features for a logistic regression classifier. The logistic regression classifier is trained on various tasks in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.", "The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.", "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks: itemize MR: Sentiment prediction for movie reviews snippets on a five start scale .", "The results can be found in Table . SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.", "It appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder.", "The only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.", "Average BERT embeddings or using the CLS-token output from a BERT network achieved bad results for various STS tasks (Table ), worse than average GloVe embeddings. However, for Sent-Eval, average BERT embeddings and the BERT CLS-token output achieves decent results (Table ), outperforming average GloVe embeddings. The reason for this are the different setups. For the STS tasks, we used cosine-similarity to estimate the similarities between sentence embeddings. Cosine-similarity treats all dimensions equally. In contrast, SentEval fits a logistic regression classifier to the sentence embeddings. This allows that certain dimensions can have higher or lower impact on the classification result.", "We conclude that average BERT embeddings / CLS-token output from BERT return sentence embeddings that are infeasible to be used with cosine-similarity or with Manhatten / Euclidean distance. For transfer learning, they yield slightly worse results than InferSent or Universal Sentence Encoder. However, using the described fine-tuning setup with a siamese network structure on NLI datasets yields sentence embeddings that achieve a new state-of-the-art for the SentEval toolkit.", "We have demonstrated strong empirical results for the quality of SBERT sentence embeddings. In this section, we perform an ablation study of different aspects of SBERT in order to get a better understanding of their relative importance.", "We evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances.", "The objective function (classification vs. regression) depends on the annotated dataset. For the classification objective function, we train SBERT-base on the SNLI and the Multi-NLI dataset. For the regression objective function, we train on the training set of the STS benchmark dataset. Performances are measured on the development split of the STS benchmark dataset. Results are shown in Table .", "When trained with the classification objective function on NLI data, the pooling strategy has a rather minor impact. The impact of the concatenation mode is much larger. InferSent and Universal Sentence Encoder both use (u, v, |u-v|, u*v) as input for a softmax classifier. However, in our architecture, adding the element-wise u*v decreased the performance.", "The most important component is the element-wise difference |u-v|. Note, that the concatenation mode is only relevant for training the softmax classifier. At inference, when predicting similarities for the STS benchmark dataset, only the sentence embeddings u and v are used in combination with cosine-similarity. The element-wise difference measures the distance between the dimensions of the two sentence embeddings, ensuring that similar pairs are closer and dissimilar pairs are further apart.", "When trained with the regression objective function, we observe that the pooling strategy has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to , who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling.", "Sentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent , and Universal Sentence Encoder .", "For our comparison we use the sentences from the STS benchmark . We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent is based on PyTorch. For Universal Sentence Encoder, we use the TensorFlow Hub version, which is based on TensorFlow. SBERT is based on PyTorch. For improved computation of sentence embeddings, we implemented a smart batching strategy: Sentences with similar lengths are grouped together and are only padded to the longest element in a mini-batch. This drastically reduces computational overhead from padding tokens.", "Performances were measured on a server with Intel i7-5820K CPU @ 3.30GHz, Nvidia Tesla V100 GPU, CUDA 9.2 and cuDNN. The results are depicted in Table .", "On CPU, InferSent is about 65% faster than SBERT. This is due to the much simpler network architecture. InferSent uses a single Bi-LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efficiency on GPUs. There, SBERT with smart batching is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. Smart batching achieves a speed-up of 89% on CPU and 48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings.", "We showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings.", "To overcome this shortcoming, we presented Sentence-BERT (SBERT). SBERT fine-tunes BERT in a siamese / triplet network architecture. We evaluated the quality on various common benchmarks, where it could achieve a significant improvement over state-of-the-art sentence embeddings methods. Replacing BERT with RoBERTa did not yield a significant improvement in our experiments.", "SBERT is computationally efficient. On a GPU, it is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. SBERT can be used for tasks which are computationally not feasible to be modeled with BERT. For example, clustering of 10,000 sentences with hierarchical clustering requires with BERT about 65 hours, as around 50 Million sentence combinations must be computed. With SBERT, we were able to reduce the effort to about 5 seconds."]